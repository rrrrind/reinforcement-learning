{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 200%;\">Sarsa</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/3/19\n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Sarsaの理論・仕組みを知る．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> SarsaによるMountainCarの実装を行う．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsaとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Sarsaとは，[時間的差分学習(Temporal Difference Learning; TD学習)]()を用いた<b>最適方策決定</b>を行うための手法の1つである．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> TD学習を軽く説明すると，シミュレーションを行うことで状態・行動価値を求めるため，確率遷移がわからない問題に対して適用することができる点(モンテカルロ法の利点)と，ブートストラップを使うことができるため，エピソードの終点まで待たなくても状態・行動価値を更新することができる点(動的計画法の利点)の2つの利点を持つ<u>状態・行動価値関数更新手法</u>である．式で表すと，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">\n",
    "$$\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha\\bigl[r_{t+1} + \\gamma \\cdot V(s_{t+1}) - V(s_t)\\bigr]，\\tag{1}\n",
    "$$\n",
    "<span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> と表される．行動価値は式(1)を，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">\n",
    "\\begin{eqnarray}\n",
    "    Q(s_t,a_t) &\\leftarrow& Q(s_t,a_t) + \\alpha\\bigl[r_{t+1} + \\gamma \\cdot Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\bigr]，\\tag{2} \\\\\n",
    "    Q(s,a) &\\leftarrow& Q(s,a) + \\alpha\\bigl[r' + \\gamma \\cdot Q(s',a') - Q(s,a)\\bigr]，\\tag{3}\n",
    "\\end{eqnarray}\n",
    "<span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> のように書き換えることで導出することが可能となる．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> \n",
    "Sarsaでは，状態価値より行動価値を求めたほうが都合が良いため，式(3)と$\\epsilon$-greedy法を用いることでエピソード(方策)の更新を行う．\n",
    "具体的には，方策を決定した後にシミュレーション(行動)を行い，式(3)を用いることで行動価値を更新し，最後に<font color=\"red\">$\\epsilon$-greedy法</font>によって方策を更新する．\n",
    "ここで注意すべき点は，<b>方策更新に用いるエピソードは，その時に行ったエピソードである</b>．\n",
    "</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:08:16.247809Z",
     "start_time": "2020-03-19T17:08:16.243065Z"
    }
   },
   "source": [
    "<img src=\"figure1.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 上の画像を用いて説明すると，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 1. 初めに$\\epsilon$-greedy法で方策を決定する．今回は仮に赤線のルートになったとする．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 2. 次に行動$a''$で失敗し，状態$S'''$で止まったとする．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 3. その時の行動価値の更新は，赤線の行動が保有する行動価値を用いて，赤線の行動が保有する行動価値の更新を行う．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 4. 最後に，行動価値が更新されたので再度$\\epsilon$-greedy法を用いて方策を決定する．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> よくわからなかった人は，[Q-Learning]()とSarsaを比較してもらうとわかりやすいかもしれないです．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCarの実装における注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 試しに動かしてみる． </span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:33:36.392869Z",
     "start_time": "2020-04-14T12:33:33.283517Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# 環境の生成（車の山登り）\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# 環境の描画\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    time.sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> observation, reward, done, infoの中身を確認してみると， </span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:33:38.394412Z",
     "start_time": "2020-04-14T12:33:38.386051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "observation = [-0.4909148  -0.00531235]\n",
      "-------------------------------------------\n",
      "reward      = -1.0\n",
      "-------------------------------------------\n",
      "done        = True\n",
      "-------------------------------------------\n",
      "info        = {'TimeLimit.truncated': True}\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------\")\n",
    "print(\"observation = \" + str(observation))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"reward      = \" + str(reward))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"done        = \" + str(done))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"info        = \" + str(info))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> となっている．この時， </span> <br>\n",
    "\n",
    "<ul>\n",
    "<span style=\"font-size: 120%;\"> <li>\"observation\"はその時の状態の値を保持しており，MountainCarで説明すると，1つ目の要素が車の位置を表しており，2つ目の要素が車の速度を表している．車の位置は[min, max] = [-1.2, 0.6]であり，車の速度は[min, max] = [-0.07, 0.07]となっている．</li></span><br>\n",
    "<span style=\"font-size: 120%;\"> <li>\"reward\"は前の行動によって得られた報酬を保持しており，MountainCarでは1ステップ進む毎に-1.0の報酬が与えられるように設計されている．</li></span> <br>\n",
    "<span style=\"font-size: 120%;\"> <li>\"done\"は，ゲーム(状態)が続いているか否かをTrue or Falseで保持しており，ゲーム(状態)が続いている場合はTrueを返す．Falseはゲームをクリアした時やゲームに失敗した時など，これ以上ゲームを続けられない時に返される．</li></span> <br>\n",
    "<span style=\"font-size: 120%;\"> <li>\"info\"は，デバックする時に役立つ情報を保持している．</li></span><br>\n",
    "</ul>\n",
    "\n",
    "<span style=\"font-size: 120%;\"> 詳しい使い方は[こちら](https://gym.openai.com/docs/)と[こちら](https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16#%E5%8F%82%E8%80%83)を参照してください． </span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回も『[Cartpole]()』の時と同様に，各状態を連続値から離散値に変換する必要がある．そこで今回は各状態を40個に分割する．また，今回はQ-tableを採用するので，行動は[0, 1, 2] = [左, 何もしない, 右]の3つとなっているため，40×40×3の配列を作成し，そこに行動価値を収納する．(ここから先は合っているかわかりません)行動価値の更新方法はCartpoleの時と同じように行う．詳しくは『[Cartpole]()』の<u>3.1 行動価値関数を用いた方策評価</u>を参照してください．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:33:42.234022Z",
     "start_time": "2020-04-14T12:33:41.939906Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントクラスの設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> エージェントクラスには，『行動』『状態』『報酬』の3つの変数と実際に行動を行う『move関数』を持たせる．また，環境クラスを変えればモンテカルロ法やQ-learningでMountainCarを実装できるように設計する．</span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:33:45.617039Z",
     "start_time": "2020-04-14T12:33:45.602990Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,step_num):\n",
    "        self.step_num = step_num\n",
    "        self.actions = [0, 1, 2] # [左, 停止, 右]\n",
    "        self.states = np.zeros([self.step_num+1,2])\n",
    "        self.reward = np.zeros(self.step_num)\n",
    "        self.policy = np.random.choice(self.actions, size=self.step_num, p=[1/3,1/3,1/3])\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "    \n",
    "    def set_policy(self,policy):\n",
    "        self.policy = policy\n",
    "    \n",
    "    def move(self):\n",
    "        for i in range(self.step_num):\n",
    "            self.states[i+1], self.reward[i], done, info = env.step(self.policy[i])\n",
    "            if done == False :\n",
    "                return self.states[:i],self.reward[:i],self.policy[:i]\n",
    "        return self.states[:self.step_num],self.reward[:self.step_num],self.policy[:self.step_num]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:33:46.335132Z",
     "start_time": "2020-04-14T12:33:46.275522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.94673600e-01 -3.05443339e-04]\n",
      "---------------------\n",
      "[-5.94368157e-01  1.67764714e-04]\n",
      "---------------------\n",
      "[-0.59453592 -0.00136026]\n",
      "---------------------\n",
      "[-0.59317566 -0.0008783 ]\n",
      "---------------------\n",
      "[-0.59229736 -0.0023899 ]\n",
      "---------------------\n",
      "[-0.58990746 -0.00188394]\n",
      "---------------------\n",
      "[-0.58802351 -0.00336413]\n",
      "---------------------\n",
      "[-0.58465939 -0.00381952]\n",
      "---------------------\n",
      "[-0.58083986 -0.00424673]\n",
      "---------------------\n",
      "[-0.57659314 -0.00564251]\n",
      "---------------------\n",
      "[-0.57095063 -0.00699646]\n",
      "---------------------\n",
      "[-0.56395417 -0.00729838]\n",
      "---------------------\n",
      "[-0.55665578 -0.00754591]\n",
      "---------------------\n",
      "[-0.54910988 -0.00673705]\n",
      "---------------------\n",
      "[-0.54237283 -0.00787778]\n",
      "---------------------\n",
      "[-0.53449504 -0.00895949]\n",
      "---------------------\n",
      "[-0.52553555 -0.00897402]\n",
      "---------------------\n",
      "[-0.51656154 -0.00892124]\n",
      "---------------------\n",
      "[-0.5076403 -0.0098016]\n",
      "---------------------\n",
      "[-0.4978387  -0.01060859]\n",
      "---------------------\n",
      "[-0.48723011 -0.01133637]\n",
      "---------------------\n",
      "[-0.47589374 -0.0119798 ]\n",
      "---------------------\n",
      "[-0.46391395 -0.01053455]\n",
      "---------------------\n",
      "[-0.4533794  -0.00901179]\n",
      "---------------------\n",
      "[-0.44436761 -0.00942314]\n",
      "---------------------\n",
      "[-0.43494447 -0.00876604]\n",
      "---------------------\n",
      "[-0.42617843 -0.00704575]\n",
      "---------------------\n",
      "[-0.41913268 -0.00727501]\n",
      "---------------------\n",
      "[-0.41185767 -0.00745256]\n",
      "---------------------\n",
      "[-0.40440511 -0.00657754]\n",
      "---------------------\n",
      "[-0.39782757 -0.00465648]\n",
      "---------------------\n",
      "[-0.39317109 -0.00270304]\n",
      "---------------------\n",
      "[-0.39046805 -0.0027309 ]\n",
      "---------------------\n",
      "[-0.38773716 -0.00273991]\n",
      "---------------------\n",
      "[-0.38499724 -0.0027301 ]\n",
      "---------------------\n",
      "[-0.38226715 -0.00170158]\n",
      "---------------------\n",
      "[-0.38056556 -0.00066145]\n",
      "---------------------\n",
      "[-0.37990411 -0.00061681]\n",
      "---------------------\n",
      "[-0.37928729  0.00143203]\n",
      "---------------------\n",
      "[-0.38071932  0.00347111]\n",
      "---------------------\n",
      "[-0.38419043  0.00548646]\n",
      "---------------------\n",
      "[-0.38967689  0.00546407]\n",
      "---------------------\n",
      "[-0.39514095  0.00640383]\n",
      "---------------------\n",
      "[-0.40154478  0.00629892]\n",
      "---------------------\n",
      "[-0.4078437   0.00814973]\n",
      "---------------------\n",
      "[-0.41599343  0.00794283]\n",
      "---------------------\n",
      "[-0.42393626  0.00967921]\n",
      "---------------------\n",
      "[-0.43361547  0.00934592]\n",
      "---------------------\n",
      "[-0.44296139  0.00994482]\n",
      "---------------------\n",
      "[-0.45290621  0.01047105]\n",
      "---------------------\n",
      "[-0.46337726  0.00992026]\n",
      "---------------------\n",
      "[-0.47329752  0.01129609]\n",
      "---------------------\n",
      "[-0.4845936   0.01158796]\n",
      "---------------------\n",
      "[-0.49618156  0.01279335]\n",
      "---------------------\n",
      "[-0.50897491  0.012903  ]\n",
      "---------------------\n",
      "[-0.52187791  0.0129159 ]\n",
      "---------------------\n",
      "[-0.53479382  0.01283196]\n",
      "---------------------\n",
      "[-0.54762577  0.01265191]\n",
      "---------------------\n",
      "[-0.56027769  0.01237737]\n",
      "---------------------\n",
      "[-0.57265506  0.01101078]\n",
      "---------------------\n",
      "[-0.58366584  0.00956271]\n",
      "---------------------\n",
      "[-0.59322855  0.00904428]\n",
      "---------------------\n",
      "[-0.60227282  0.00845968]\n",
      "---------------------\n",
      "[-0.61073251  0.0078136 ]\n",
      "---------------------\n",
      "[-0.61854611  0.00611108]\n",
      "---------------------\n",
      "[-0.62465719  0.0053647 ]\n",
      "---------------------\n",
      "[-0.63002189  0.00458002]\n",
      "---------------------\n",
      "[-0.63460191  0.0037628 ]\n",
      "---------------------\n",
      "[-0.63836471  0.00391896]\n",
      "---------------------\n",
      "[-0.64228367  0.00304751]\n",
      "---------------------\n",
      "[-0.64533118  0.00115468]\n",
      "---------------------\n",
      "[-0.64648586 -0.00074624]\n",
      "---------------------\n",
      "[-0.64573962 -0.00264193]\n",
      "---------------------\n",
      "[-0.64309769 -0.0025191 ]\n",
      "---------------------\n",
      "[-0.64057859 -0.00337855]\n",
      "---------------------\n",
      "[-0.63720004 -0.00521416]\n",
      "---------------------\n",
      "[-0.63198587 -0.00501282]\n",
      "---------------------\n",
      "[-0.62697306 -0.00577576]\n",
      "---------------------\n",
      "[-0.6211973  -0.00549733]\n",
      "---------------------\n",
      "[-0.61569997 -0.00617933]\n",
      "---------------------\n",
      "[-0.60952064 -0.00581664]\n",
      "---------------------\n",
      "[-0.603704   -0.00641166]\n",
      "---------------------\n",
      "[-0.59729234 -0.00595987]\n",
      "---------------------\n",
      "[-0.59133247 -0.00546438]\n",
      "---------------------\n",
      "[-0.58586809 -0.00592869]\n",
      "---------------------\n",
      "[-0.5799394  -0.00534924]\n",
      "---------------------\n",
      "[-0.57459016 -0.00673018]\n",
      "---------------------\n",
      "[-0.56785998 -0.00706117]\n",
      "---------------------\n",
      "[-0.56079881 -0.00633959]\n",
      "---------------------\n",
      "[-0.55445922 -0.00557071]\n",
      "---------------------\n",
      "[-0.54888851 -0.0057602 ]\n",
      "---------------------\n",
      "[-0.54312831 -0.00690659]\n",
      "---------------------\n",
      "[-0.53622172 -0.00600124]\n",
      "---------------------\n",
      "[-0.53022048 -0.0070509 ]\n",
      "---------------------\n",
      "[-0.52316958 -0.00604768]\n",
      "---------------------\n",
      "[-0.5171219  -0.00699911]\n",
      "---------------------\n",
      "[-0.51012279 -0.00589806]\n",
      "---------------------\n",
      "[-0.50422473 -0.00675284]\n",
      "---------------------\n",
      "[-0.49747189 -0.00655709]\n",
      "---------------------\n",
      "[0. 0.]\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "a = Agent(100)\n",
    "aa, b, c = a.move()\n",
    "for i,stte in enumerate(reversed(aa)):\n",
    "    print(stte)\n",
    "    print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T08:31:39.200688Z",
     "start_time": "2020-03-25T08:31:39.193532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "<reversed object at 0x7f7cc8c88518>\n"
     ]
    }
   ],
   "source": [
    "aaa = np.array([1,2,3])\n",
    "bbb = np.array([4,5,6])\n",
    "ccc = np.array([7,8,9])\n",
    "print(aaa)\n",
    "aaa = reversed(aaa)\n",
    "bbb = reversed(bbb)\n",
    "ccc = reversed(ccc)\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境クラスの設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 環境クラスでは，『状態の離散化』『Q-tableの更新』『$\\epsilon$-greedy』『方策の更新』の4つの関数を持たせる．また，Agentクラスを変えればCartpoleなどのさまざまな問題に流用できるように設計する．</span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T08:35:31.118650Z",
     "start_time": "2020-03-25T08:35:31.107259Z"
    }
   },
   "outputs": [],
   "source": [
    "class environment:\n",
    "    def __init__(self,env,actions,division,step_num=200,alpha=0.01,epsilon=0.9):\n",
    "        # division は分割数をlistで示したものである\n",
    "        # 例えばCartpoleの場合は[6,6,6,6]であり，今回は[40,40]である\n",
    "        self.env = env\n",
    "        self.division = division\n",
    "        self.q_table = np.zeros(self.division+[len(actions)])\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def discretization(self,states):\n",
    "        # states は[その時に進んだstep数, 状態数]のnp.arrayである\n",
    "        env_low = self.env.observation_space.low # 位置と速度の最小値\n",
    "        env_high = self.env.observation_space.high #　位置と速度の最大値\n",
    "        env_dx = np.array(env_high-env_low) / np.array(self.division) # 分割した時の1マス分の値を算出\n",
    "        return ((states - env_low) / env_dx).astype(np.int64)\n",
    "    \n",
    "    def q_table_update(self,states,rewards,policy):\n",
    "        for i,(state,reward,action) in enumerate(zip(reversed(states),reversed(reward),reversed(policy))):\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:45:07.332334Z",
     "start_time": "2020-03-24T19:45:07.327492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 40]\n",
      "[40, 40, 3]\n"
     ]
    }
   ],
   "source": [
    "division = [40,40]\n",
    "print(division)\n",
    "r = division + [3]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:42:50.950595Z",
     "start_time": "2020-03-24T19:42:50.945643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 40]\n",
      "[40, 40, 3]\n"
     ]
    }
   ],
   "source": [
    "l = [40,40]\n",
    "print(l)\n",
    "# [0, 1, 2]\n",
    "\n",
    "l.append(3)\n",
    "print(l)\n",
    "# [0, 1, 2, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:53:44.034292Z",
     "start_time": "2020-03-24T19:53:44.018527Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected sequence object with len >= 0 or a single integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-209cc843af39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected sequence object with len >= 0 or a single integer"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "v = environment(env)\n",
    "vv = v.discretization(aa,[40,40])\n",
    "a = np.zeros(env.action_space)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[今さら聞けない強化学習（10）: SarsaとQ学習の違い](https://qiita.com/triwave33/items/cae48e492769852aa9f1) <br>\n",
    "[深層強化学習アルゴリズムまとめ](https://qiita.com/shionhonda/items/ec05aade07b5bea78081) <br>\n",
    "[OpenAI Gym 入門](https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16#%E5%8F%82%E8%80%83) <br>\n",
    "[Getting Started with Gym](https://gym.openai.com/docs/) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Q-learningの更新式はそのルート(行動)での最大の期待値を計算している．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> つまり，イプシロン・グリーディによって方策を決定した場合，その方策(つまり行動)の現在取りうる最大の価値を計算している．したがって，行動価値を更新するときは方策を使用しない(つまり，イプシロン・グリーディを用いない)が，方策を決めるときは全てのパターンに対してシミュレーションを行わなければならないので，イプシロン・グリーディを用いて方策を決定する．方策オフというのは，更新式に方策(イプシロン・グリーディ)を用いてないからである．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> Sarsaの更新式は，そのルート(行動)の最大の期待値ではなく，仮定の状態も含まれている．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 例えば，釣りで魚を釣り上げることを考えた場合に，Q-learningの場合は，リールを力いっぱい巻けば魚は釣れるので行動価値も高くなるが，Sarsaの場合は，糸が切れることなども考慮されるので，力いっぱい引くのではなく，強弱をつけたりなどの保険要素が付け加えられる．</span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
