{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 200%;\">Sarsa</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/3/19\n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Sarsaの理論・仕組みを知る．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> SarsaによるMountainCarの実装を行う．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsaとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Sarsaとは，[時間的差分学習(Temporal Difference Learning; TD学習)]()を用いた<b>最適方策決定</b>を行うための手法の1つである．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> TD学習を軽く説明すると，シミュレーションを行うことで状態・行動価値を求めるため，確率遷移がわからない問題に対して適用することができる点(モンテカルロ法の利点)と，ブートストラップを使うことができるため，エピソードの終点まで待たなくても状態・行動価値を更新することができる点(動的計画法の利点)の2つの利点を持つ<u>状態・行動価値関数更新手法</u>である．式で表すと，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">\n",
    "$$\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha\\bigl[r_{t+1} + \\gamma \\cdot V(s_{t+1}) - V(s_t)\\bigr]，\\tag{1}\n",
    "$$\n",
    "<span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> と表される．行動価値は式(1)を，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">\n",
    "\\begin{eqnarray}\n",
    "    Q(s_t,a_t) &\\leftarrow& Q(s_t,a_t) + \\alpha\\bigl[r_{t+1} + \\gamma \\cdot Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\bigr]，\\tag{2} \\\\\n",
    "    Q(s,a) &\\leftarrow& Q(s,a) + \\alpha\\bigl[r' + \\gamma \\cdot Q(s',a') - Q(s,a)\\bigr]，\\tag{3}\n",
    "\\end{eqnarray}\n",
    "<span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> のように書き換えることで導出することが可能となる．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> \n",
    "Sarsaでは，状態価値より行動価値を求めたほうが都合が良いため，式(3)と[$\\epsilon$-greedy法]()を用いることでエピソード(方策)の更新を行う．\n",
    "具体的には，方策を決定した後にシミュレーション(行動)を行い，式(3)を用いることで行動価値を更新し，最後に<font color=\"red\">$\\epsilon$-greedy法</font>によって方策を更新する．\n",
    "ここで注意すべき点は，<b>方策更新に用いるエピソードは，その時に行ったエピソードである</b>．\n",
    "</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:08:16.247809Z",
     "start_time": "2020-03-19T17:08:16.243065Z"
    }
   },
   "source": [
    "<img src=\"figure1.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 上の画像を用いて説明すると，</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 1. 初めに$\\epsilon$-greedy法で方策を決定する．今回は仮に赤線のルートになったとする．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 2. 次に行動$a''$で失敗し，状態$S'''$で止まったとする．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 3. その時の行動価値の更新は，赤線の行動が保有する行動価値を用いて，赤線の行動が保有する行動価値の更新を行う．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 4. 最後に，行動価値が更新されたので再度$\\epsilon$-greedy法を用いて方策を決定する．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> よくわからなかった人は，[Q-Learning]()とSarsaを比較してもらうとわかりやすいかもしれないです．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCarの実装における注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 試しに動かしてみる． </span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:25:58.238627Z",
     "start_time": "2020-03-24T06:25:55.640891Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# 環境の生成（車の山登り）\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# 環境の描画\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> この時， </span> <br>\n",
    "<span style=\"font-size: 120%;\"> \"observation(object)\"は，その時の状態の値を保持しており，  </span> <br>\n",
    "<span style=\"font-size: 120%;\"> \"reward(float)\"は，前の行動によって得られた報酬を保持しており，  </span> <br>\n",
    "<span style=\"font-size: 120%;\"> \"done(boolean)\"は，  </span> <br>\n",
    "<span style=\"font-size: 120%;\"> \"info\"は，  </span> <br>\n",
    "<span style=\"font-size: 120%;\"> を表している．</span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 詳しい使い方は[こちら]()を参照してください． </span> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[今さら聞けない強化学習（10）: SarsaとQ学習の違い](https://qiita.com/triwave33/items/cae48e492769852aa9f1) <br>\n",
    "[深層強化学習アルゴリズムまとめ](https://qiita.com/shionhonda/items/ec05aade07b5bea78081) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Q-learningの更新式はそのルート(行動)での最大の期待値を計算している．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> つまり，イプシロン・グリーディによって方策を決定した場合，その方策(つまり行動)の現在取りうる最大の価値を計算している．したがって，行動価値を更新するときは方策を使用しない(つまり，イプシロン・グリーディを用いない)が，方策を決めるときは全てのパターンに対してシミュレーションを行わなければならないので，イプシロン・グリーディを用いて方策を決定する．方策オフというのは，更新式に方策(イプシロン・グリーディ)を用いてないからである．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> Sarsaの更新式は，そのルート(行動)の最大の期待値ではなく，仮定の状態も含まれている．</span> <br>\n",
    "<span style=\"font-size: 120%;\"> 例えば，釣りで魚を釣り上げることを考えた場合に，Q-learningの場合は，リールを力いっぱい巻けば魚は釣れるので行動価値も高くなるが，Sarsaの場合は，糸が切れることなども考慮されるので，力いっぱい引くのではなく，強弱をつけたりなどの保険要素が付け加えられる．</span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
