{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "<span style=\"font-size: 200%;\">状態価値関数 (State Value Function)</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2019/12/20 \n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![問題設定](figure/ProblemSetting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\"> 今回は5×5の格子世界(Grid World)における各マス目の状態価値を求める</span>\n",
    "- <span style=\"font-size: 120%;\"> エージェントである棒人間は前後左右に動くことができる．また今回はスタート地点を考えないものとする．つまり，現在エージェントは(1,2)にいるが，ここがスタートという訳ではなく，あるマス目でスタートした時のそのマス目の状態価値を求めるということである</span>\n",
    "- <span style=\"font-size: 120%;\"> 報酬はポイントマスで何かしらの行動を取った時に得ることができ，報酬を獲得した後は強制的にワープマスへ飛ばされる</span>\n",
    "- <span style=\"font-size: 120%;\"> 境界線の外側には出れないこととする．仮に外に出た場合，-1ptの報酬とする．またその時の位置は，境界線を出る1つ前の位置に戻されることとする</span>\n",
    "- <span style=\"font-size: 120%;\"> 状態価値は，\n",
    "    $${\\rm V^{\\pi}}(s) = \\Sigma_{a}{\\rm \\pi} (s,a)\\Sigma_{s'}{\\rm P}^{a}_{s,s'}\\bigr[ {\\rm R}^{a}_{s,s'} + \\gamma {\\rm V^{\\pi}}(s') \\bigl], \\nonumber$$\n",
    "によって算出する</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルコフ決定過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\">状態s&isin;S: 現在のマス目(x,y)</span>  \n",
    "- <span style=\"font-size: 120%;\">行動a&isin;A: エージェントの上下左右の動き { (0,1) (0,-1) (-1,0) (1,0) }</span>  \n",
    "- <span style=\"font-size: 120%;\">確率遷移T(s,a,s'){ = P(s'|s,a) }: 今回は確定的に\"1\"とする(右への気分が60%,左への気分は40%みたいのは無し) </span>  \n",
    "- <span style=\"font-size: 120%;\">報酬R(s,a,s'): </span>\n",
    "    $$\n",
    "    {\\rm R(s,a,s')} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    -1pt & (境界線外へ移動) \\\\\n",
    "    5pt & (5pt報酬マスで行動) \\\\\n",
    "    10pt & (10pt報酬マスで行動) \\\\\n",
    "    0pt & (上記以外)\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$ <br>\n",
    "- <span style=\"font-size: 120%;\">方策&pi;(s,a): </span>\n",
    "    $$\n",
    "    {\\rm \\pi(s,a)} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    0.25 & (any, up) \\\\\n",
    "    0.25 & (any, down) \\\\\n",
    "    0.25 & (any, right) \\\\\n",
    "    0.25 & (any, left)\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$  \n",
    "\n",
    "<span style=\"font-size: 120%;\"> ※そもそも『マルコフ決定過程って何?』という方は，[こちら](https://www.hellocybernetics.tech/entry/2019/06/24/192311)を読んでください． </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonでの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~とりあえず実装してみる~~ ⇒ 3.4(ソースコードの修正)へGO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = [[0,4],[1,4],[2,4],[3,4],[4,4],\n",
    "         [0,3],[1,3],[2,3],[3,3],[4,3],\n",
    "         [0,2],[1,2],[2,2],[3,2],[4,2],\n",
    "         [0,1],[1,1],[2,1],[3,1],[4,1],\n",
    "         [0,0],[1,0],[2,0],[3,0],[4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): # stage_map, the array number of the stage    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # 行動a,方策πの宣言。引数で使うので、初めに宣言する。\n",
    "        self.actions = [[0,1],[0,-1],[-1,0],[1,0]] # up,down,left,right\n",
    "        self.each_pi = [0.25,0.25,0.25,0.25]\n",
    "        self.position = []\n",
    "    \n",
    "    def set_pos(self,now_pos):\n",
    "        # 現在地の更新\n",
    "        self.position = now_pos\n",
    "        \n",
    "    def get_pos(self):\n",
    "        # 現在地の取得\n",
    "        return self.position\n",
    "    \n",
    "    def move(self,action):\n",
    "        # 行動後の位置の取得\n",
    "        if self.get_pos() == [1,4]: # 10ptゾーン\n",
    "            next_pos = [1,0]\n",
    "        elif self.get_pos() == [3,4]: # 5ptゾーン\n",
    "            next_pos = [3,2]\n",
    "        else :\n",
    "            next_pos = [self.get_pos()[0] + action[0], self.get_pos()[1] + action[1]]\n",
    "        \n",
    "        # 境界線外に出ている時の処理\n",
    "        if 0 > self.get_pos()[0] or self.get_pos()[0] > 4 or 0 > self.get_pos()[1] or self.get_pos()[1] > 4:\n",
    "            next_pos = self.get_pos()\n",
    "            \n",
    "        self.set_pos(next_pos)\n",
    "    \n",
    "    def pi(self,state,num):\n",
    "        return self.each_pi[num]\n",
    "        \n",
    "    def reward(self,state,action):\n",
    "        if state == [1,4]:\n",
    "            return 10\n",
    "        \n",
    "        if state == [3,4]:\n",
    "            return 5\n",
    "        \n",
    "        if state[1] == 4 and action == [0,1]:\n",
    "            return -1\n",
    "        elif state[1] == 0 and action == [0,-1]:\n",
    "            return -1\n",
    "        elif state[0] == 0 and action == [-1,0]:\n",
    "            return -1\n",
    "        elif state[0] == 4 and action == [1,0]:\n",
    "            return -1\n",
    "        else :\n",
    "            return 0\n",
    "    \n",
    "    def v_pi(self,state,n,out,iter_num): # この\"state\"は，潜った時の位置を記憶しておくため\n",
    "        if n == iter_num :\n",
    "            for i, action in enumerate(self.actions):\n",
    "                # 最下層での処理．最下層では前後左右に動いた際の報酬を一気に計算する．\n",
    "                out += self.pi(state,i) * self.reward(state,action)\n",
    "            return out\n",
    "        else :\n",
    "            for i, action in enumerate(self.actions):\n",
    "                # 先に π*R を計算し，action方向に移動する\n",
    "                out += self.pi(self.get_pos(),i) * self.reward(self.get_pos(),action)\n",
    "                self.move(action)\n",
    "                # この時、\"state\"は1つ前の位置，\"self.get_pos()\"には現在の位置が記憶されている\n",
    "                # 移動後，つまり現在の状態はs'となっているので，s'の報酬を計算する\n",
    "                out += self.pi(self.get_pos(),i) * 0.9 * self.v_pi(self.get_pos(), n+1, 0, iter_num)\n",
    "                # ここで再帰関数を使っているので，\"n\"が\"iter_num\"と同じ回数\n",
    "                # つまり，指定した深さまで潜らないと次のプログラムへ進むことはできない\n",
    "                self.set_pos(state) # \"self.get_pos()\"から1つ前の状態\"state\"に戻るため\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61bd557aea54c1a9d96769dd81d3f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elapsed_time:945.1679182052612[sec]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "v_pi_value = []\n",
    "\n",
    "start = time.time()\n",
    "for i,locate in tqdm(enumerate(stage)):\n",
    "    agent.set_pos(locate)\n",
    "    v_pi_value.append(agent.v_pi(agent.get_pos(),0,0,11))\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5 , 10.  , -0.25,  5.  , -0.5 ],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.5 , -0.25, -0.25, -0.25, -0.5 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = np.array(v_pi_value)\n",
    "outcome = outcome.reshape(5,5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果・考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![実行結果](figure/outcome.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\"> 全体的にスコアが低い気がする</span>  \n",
    "<span style=\"font-size: 120%;\"> ⇒ 報酬に関する記述間違えてる?</span>  \n",
    "- <span style=\"font-size: 120%;\"> 値は間違えてるけど、10ptや5pt付近のエリアの状態価値が高くなっている</span>  \n",
    "<span style=\"font-size: 120%;\"> ⇒ 大まかには合ってそう．少なくとも再帰は合ってるはず</span>  \n",
    "- <span style=\"font-size: 120%;\"> 実行時間がやばい(再帰11回で15分強)。</span>  \n",
    "<span style=\"font-size: 120%;\"> ⇒ 再帰+for文は時間がかかる(動的計画法の問題点)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1つずつ確認していこう!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 初めに，再帰無しの場合を考える． </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5 , 10.  , -0.25,  5.  , -0.5 ],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.25,  0.  ,  0.  ,  0.  , -0.25],\n",
       "       [-0.5 , -0.25, -0.25, -0.25, -0.5 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "v_pi_value = []\n",
    "for i,locate in enumerate(stage):\n",
    "    agent.set_pos(locate)\n",
    "    v_pi_value.append(agent.v_pi(agent.get_pos(),0,0,0))\n",
    "outcome = np.array(v_pi_value)\n",
    "outcome = outcome.reshape(5,5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">(0,4) と(1,4)の価値関数を計算をしてみると，</span> </br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    0.25 \\times -1 + 0.25 \\times  0 + 0.25 \\times -1 + 0.25 \\times  0 &=& -0.5, \\\\\n",
    "    0.25 \\times 10 + 0.25 \\times 10 + 0.25 \\times 10 + 0.25 \\times 10 &=&   10\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<span style=\"font-size: 120%;\">となり，合っていることがわかる(計算順は上下左右)．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">続いて，再帰1回の場合を考える．</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.58125,  9.775  ,  3.125  ,  5.     ,  0.45625],\n",
       "       [-0.41875,  2.19375, -0.05625,  1.06875, -0.41875],\n",
       "       [-0.3625 , -0.05625,  0.     , -0.05625, -0.3625 ],\n",
       "       [-0.41875, -0.1125 , -0.05625, -0.1125 , -0.41875],\n",
       "       [-0.725  , -0.41875, -0.3625 , -0.41875, -0.725  ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "v_pi_value = []\n",
    "for i,locate in enumerate(stage):\n",
    "    agent.set_pos(locate)\n",
    "    v_pi_value.append(agent.v_pi(agent.get_pos(),0,0,1))\n",
    "outcome = np.array(v_pi_value)\n",
    "outcome = outcome.reshape(5,5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">(0,4) と(1,4)の価値関数を計算をしてみると，</span> </br>\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& 0.25 \\times \\bigl\\{ -1 + 0.9\\times(0.25 \\times -1 + 0.25 \\times  \\;\\,0 + 0.25 \\times -1 + 0.25 \\times \\;\\,0 ) \\bigr\\} \\\\\n",
    "& 0.25 \\times \\bigl\\{\\;\\,\\,0 + 0.9\\times(0.25 \\times \\;\\,\\,0 + 0.25 \\times  \\;\\,0 + 0.25 \\times -1 + 0.25 \\times \\;\\,0 ) \\bigr\\} \\\\\n",
    "& 0.25 \\times \\bigl\\{ -1 + 0.9\\times(0.25 \\times -1 + 0.25 \\times  \\;\\,0 + 0.25 \\times -1 + 0.25 \\times \\;\\,0 ) \\bigr\\} \\\\\n",
    "& 0.25 \\times \\bigl\\{\\;\\,\\,0 + 0.9\\times(0.25 \\times \\,10 + 0.25 \\times 10 + 0.25 \\times \\,10 + 0.25 \\times 10 ) \\bigr\\} = 1.46875\n",
    "\\end{split} \\nonumber\n",
    "$$\n",
    "\n",
    "<span style=\"font-size: 120%;\">となり，間違っていることが分かる(計算順は上下左右)．</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.46875"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*(-1 + 0.9 *(0.25*-1 + 0.25*0 + 0.25*-1 + 0.25*0)) + \\\n",
    "0.25*(0 + 0.9 *(0.25*0 + 0.25*0 + 0.25*-1 + 0.25*0)) + \\\n",
    "0.25*(-1 + 0.9 *(0.25*-1 + 0.25*0 + 0.25*-1 + 0.25*0)) + \\\n",
    "0.25*(0 + 0.9 *(0.25*10 + 0.25*10 + 0.25*10 + 0.25*10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">ここで，上記の式の1行目または3行目における再帰箇所の\"-1\"を全て\"0\"に置き換えると，</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.58125"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25*(-1 + 0.9 *(0.25*-1 + 0.25*0 + 0.25*-1 + 0.25*0)) + \\\n",
    "0.25*(0 + 0.9 *(0.25*0 + 0.25*0 + 0.25*-1 + 0.25*0)) + \\\n",
    "0.25*(-1 + 0.9 *(0.25*0 + 0.25*0 + 0.25*0 + 0.25*0)) + \\\n",
    "0.25*(0 + 0.9 *(0.25*10 + 0.25*10 + 0.25*10 + 0.25*10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> となり，(0,4)の状態価値と値が同じになる． </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">これらのことから，境界線を超えた時に対する処理の記述が間違えているということが分かる．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソースコードの修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = [[0,4],[1,4],[2,4],[3,4],[4,4],\n",
    "         [0,3],[1,3],[2,3],[3,3],[4,3],\n",
    "         [0,2],[1,2],[2,2],[3,2],[4,2],\n",
    "         [0,1],[1,1],[2,1],[3,1],[4,1],\n",
    "         [0,0],[1,0],[2,0],[3,0],[4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): # stage_map, the array number of the stage    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # 行動a,方策πの宣言。引数で使うので、初めに宣言する。\n",
    "        self.actions = [[0,1],[0,-1],[-1,0],[1,0]] # up,down,left,right\n",
    "        self.each_pi = [0.25,0.25,0.25,0.25]\n",
    "        self.position = []\n",
    "    \n",
    "    def set_pos(self,now_pos):\n",
    "        # 現在地の更新\n",
    "        self.position = now_pos\n",
    "        \n",
    "    def get_pos(self):\n",
    "        # 現在地の取得\n",
    "        return self.position\n",
    "    \n",
    "    def move(self,action):\n",
    "        # 行動後の位置の取得\n",
    "        if self.get_pos() == [1,4]: # 10ptゾーン\n",
    "            next_pos = [1,0]\n",
    "        elif self.get_pos() == [3,4]: # 5ptゾーン\n",
    "            next_pos = [3,2]\n",
    "        else :\n",
    "            next_pos = [self.get_pos()[0] + action[0], self.get_pos()[1] + action[1]]\n",
    "        \n",
    "        # 境界線外に出ている時の処理\n",
    "        if 0 > next_pos[0] or next_pos[0] > 4 or 0 > next_pos[1] or next_pos[1] > 4:\n",
    "            next_pos = self.get_pos()\n",
    "            \n",
    "        self.set_pos(next_pos)\n",
    "    \n",
    "    def pi(self,state,num):\n",
    "        return self.each_pi[num]\n",
    "        \n",
    "    def reward(self,state,action):\n",
    "        if state == [1,4]:\n",
    "            return 10\n",
    "        \n",
    "        if state == [3,4]:\n",
    "            return 5\n",
    "        \n",
    "        if state[1] == 4 and action == [0,1]:\n",
    "            return -1\n",
    "        elif state[1] == 0 and action == [0,-1]:\n",
    "            return -1\n",
    "        elif state[0] == 0 and action == [-1,0]:\n",
    "            return -1\n",
    "        elif state[0] == 4 and action == [1,0]:\n",
    "            return -1\n",
    "        else :\n",
    "            return 0\n",
    "    \n",
    "    def v_pi(self,state,n,out,iter_num): # この\"state\"は，潜った時の位置を記憶しておくため\n",
    "        if n == iter_num :\n",
    "            for i, action in enumerate(self.actions):\n",
    "                # 最下層での処理．最下層では前後左右に動いた際の報酬を一気に計算する．\n",
    "                out += self.pi(state,i) * self.reward(state,action)\n",
    "            return out\n",
    "        else :\n",
    "            for i, action in enumerate(self.actions):\n",
    "                # 先に π*R を計算し，action方向に移動する\n",
    "                out += self.pi(self.get_pos(),i) * self.reward(self.get_pos(),action)\n",
    "                self.move(action)\n",
    "                # この時、\"state\"は1つ前の位置，\"self.get_pos()\"には現在の位置が記憶されている\n",
    "                # 移動後，つまり現在の状態はs'となっているので，s'の報酬を計算する\n",
    "                out += self.pi(self.get_pos(),i) * 0.9 * self.v_pi(self.get_pos(), n+1, 0, iter_num)\n",
    "                # ここで再帰関数を使っているので，\"n\"が\"iter_num\"と同じ回数\n",
    "                # つまり，指定した深さまで潜らないと次のプログラムへ進むことはできない\n",
    "                self.set_pos(state) # \"self.get_pos()\"から1つ前の状態\"state\"に戻るため\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17b983ceafb4a439a31baa0a5929a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elapsed_time:816.9026861190796[sec]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "v_pi_value = []\n",
    "\n",
    "start = time.time()\n",
    "for i,locate in tqdm(enumerate(stage)):\n",
    "    agent.set_pos(locate)\n",
    "    v_pi_value.append(agent.v_pi(agent.get_pos(),0,0,11))\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.34372942,  8.86087598,  4.44351842,  5.31948271,  1.43977865],\n",
       "       [ 1.5304277 ,  3.01166897,  2.24619831,  1.88552175,  0.50419075],\n",
       "       [ 0.05982435,  0.74571609,  0.68446781,  0.354529  , -0.40967115],\n",
       "       [-0.94315215, -0.39541655, -0.3191193 , -0.53994032, -1.1414136 ],\n",
       "       [-1.80039152, -1.28475124, -1.15821025, -1.34834579, -1.89474334]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = np.array(v_pi_value)\n",
    "outcome = outcome.reshape(5,5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![修正後実行結果](figure/fixed_outcome.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![まとめ](figure/conclusion.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回は状態価値関数を用いた各マス目の状態価値の導出を行った．</span>  \n",
    "<span style=\"font-size: 120%;\"> 次回は，[行動価値関数を用いた行動価値の導出](https://github.com/rrrrind/reinforcement-learning/blob/master/DP/src/Action%20Value%20Function/Action_Value_Function.ipynb)を行う．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [強化学習の基礎：マルコフ決定過程ってなんぞ？](https://www.hellocybernetics.tech/entry/2019/06/24/192311)\n",
    "- [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)\n",
    "- [今さら聞けない強化学習（2）：状態価値関数の実装](https://qiita.com/triwave33/items/3bad9f35d213a315ce78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": " -1 + 0.25 \\times  0 + 0.25 \\times -1 + 0.25 \\times 0 ), \\nonumber\\end{equation",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
