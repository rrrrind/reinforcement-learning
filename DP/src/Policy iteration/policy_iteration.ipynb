{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "<span style=\"font-size: 200%;\">方策反復(Policy iteration)</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/1/10 \n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回は方策反復によって，方策の更新を行う． </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策反復(Policy iteration)とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 方策反復とは，\n",
    "    <span style=\"color: red\"> **反復方策評価(Iterative Policy Evaluation)**</span>\n",
    "    によって算出した状態価値から\n",
    "    <span style=\"color: red\"> **方策改善(Policy improvement)**</span>\n",
    "    により，ある方策下での行動報酬が最大となるように方策を更新していくことである．</span>\n",
    "<span style=\"font-size: 120%;\"> より分かりやすく説明すると， </span>  \n",
    "\n",
    "1. <span style=\"font-size: 120%;\"> ある方策$\\pi$が存在するとき </span>\n",
    "1. <span style=\"font-size: 120%;\"> その方策$\\pi$に従ったときの状態価値を算出し(反復方策評価) </span>\n",
    "1. <span style=\"font-size: 120%;\"> その状態時における最適な方策$\\pi^*$を算出する(方策改善) </span>\n",
    "\n",
    "<span style=\"font-size: 120%;\"> ということであり，1～3を繰り返すことを\n",
    "    <span style=\"color: red\">**方策反復**(Policy iteration)</span>\n",
    "    という．</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反復方策評価(Iterative Policy Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> これは，今まで再帰処理によって状態価値を求めていたが，時間がかかるので，反復処理を用いることで状態価値関数の近似を試みるということである．  \n",
    "前回実装したので，気になる方は[こちら](https://github.com/rrrrind/reinforcement-learning/blob/master/DP/src/Iterative%20Policy%20Evaluation/Iterative_Policy_Evaluation.ipynb)を参照してください．\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策改善(Policy improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回はここがメインです． </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![方策改善](figure/policy_improvement.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 詳細は[こちら](https://qiita.com/triwave33/items/59768d14da38f50fb76c#%E6%96%B9%E7%AD%96%E5%8F%8D%E5%BE%A9)． </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonでの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step1 : 初期化 ----------\n",
    "N = 1000\n",
    "Gamma = 0.9\n",
    "\n",
    "# 格子世界の大きさ\n",
    "x_axis = 5\n",
    "y_axis = 5\n",
    "\n",
    "# 現在の価値関数と更新後の価値関数の初期化\n",
    "V      = np.zeros(x_axis * y_axis)\n",
    "V_next = np.zeros([N, x_axis * y_axis])\n",
    "\n",
    "# 現在の方策と更新後の方策の初期化\n",
    "pi      = np.zeros(x_axis * y_axis, dtype=\"int\")\n",
    "pi_next = np.zeros([N, x_axis * y_axis], dtype=\"int\")\n",
    "\n",
    "# 格子世界の構築\n",
    "stage = [[0,4],[1,4],[2,4],[3,4],[4,4],\n",
    "         [0,3],[1,3],[2,3],[3,3],[4,3],\n",
    "         [0,2],[1,2],[2,2],[3,2],[4,2],\n",
    "         [0,1],[1,1],[2,1],[3,1],[4,1],\n",
    "         [0,0],[1,0],[2,0],[3,0],[4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの設計\n",
    "# エージェントには，『状態』『行動』『報酬』『方策』を持たせている\n",
    "\n",
    "class Agent():    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # 行動aの宣言。引数で使うので、初めに宣言する。\n",
    "        self.actions = [[0,1],[0,-1],[-1,0],[1,0]] # up,down,left,right\n",
    "    \n",
    "        self.position = []\n",
    "        \n",
    "        # 方策は『各状態時での行動』を持たせている\n",
    "        # 0～3は『up,down,left,right』を示している\n",
    "        self.pi = np.random.randint(0,4,25)\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def set_pos(self,now_pos):\n",
    "        # 現在地の更新\n",
    "        self.position = now_pos\n",
    "        \n",
    "    def get_pos(self):\n",
    "        # 現在地の取得\n",
    "        return self.position\n",
    "    \n",
    "    def set_pi(self,new_pi):\n",
    "        self.pi = new_pi\n",
    "    \n",
    "    def get_pi(self):\n",
    "        return self.pi\n",
    "    \n",
    "    def move(self,action):\n",
    "        # 行動後の位置の取得\n",
    "        if self.get_pos() == [1,4]: # 10ptゾーン\n",
    "            next_pos = [1,0]\n",
    "        elif self.get_pos() == [3,4]: # 5ptゾーン\n",
    "            next_pos = [3,2]\n",
    "        else :\n",
    "            next_pos = [self.get_pos()[0] + action[0], self.get_pos()[1] + action[1]]\n",
    "        \n",
    "        # 境界線外に出ている時の処理\n",
    "        if 0 > next_pos[0] or next_pos[0] > 4 or 0 > next_pos[1] or next_pos[1] > 4:\n",
    "            next_pos = self.get_pos()\n",
    "            \n",
    "        self.set_pos(next_pos)\n",
    "        \n",
    "    def reward(self,state,action):\n",
    "        if state == [1,4]:\n",
    "            return 10\n",
    "        \n",
    "        if state == [3,4]:\n",
    "            return 5\n",
    "        \n",
    "        if state[1] == 4 and action == [0,1]:\n",
    "            return -1\n",
    "        elif state[1] == 0 and action == [0,-1]:\n",
    "            return -1\n",
    "        elif state[0] == 0 and action == [-1,0]:\n",
    "            return -1\n",
    "        elif state[0] == 4 and action == [1,0]:\n",
    "            return -1\n",
    "        else :\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ここから方策反復(Policy iteration) #####\n",
    "#        アルゴリズム通りに実装します       #\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "loop_count = 1\n",
    "pi_next[loop_count-1] = agent.get_pi()\n",
    "\n",
    "policyStable = False\n",
    "while(policyStable == False):\n",
    "    V = np.zeros(x_axis * y_axis)\n",
    "    # ---------- Step2 : 反復方策評価 ----------\n",
    "    while(True):\n",
    "        for i in range(y_axis):\n",
    "            for j in range(x_axis):\n",
    "                pos = i*5+j\n",
    "                delta = 0\n",
    "                agent.set_pos(stage[pos])\n",
    "                v = V[pos]\n",
    "                agent.move(agent.get_actions()[agent.get_pi()[pos]])\n",
    "                V[pos] = (agent.reward(stage[pos], agent.get_actions()[agent.get_pi()[pos]])\n",
    "                       + (Gamma * V[stage.index(agent.get_pos())]))\n",
    "                delta = max(delta,abs(v - V[pos]))\n",
    "        if delta < 1e-5:\n",
    "            break\n",
    "    V_next[loop_count] = V\n",
    "\n",
    "    \n",
    "    # ---------- Step3 : 方策改善 ----------\n",
    "    pi = agent.get_pi()\n",
    "    temp = np.zeros(len(agent.get_actions()))\n",
    "    for a in range(x_axis*y_axis):\n",
    "        for b, action in enumerate(agent.get_actions()):\n",
    "            agent.set_pos(stage[a])\n",
    "            agent.move(action)\n",
    "            temp[b] = (agent.reward(stage[a], action)\n",
    "                    + (Gamma * V[stage.index(agent.get_pos())]))\n",
    "        pi_next[loop_count,a] = np.argmax(temp)\n",
    "    agent.set_pi(pi_next[loop_count])\n",
    "    \n",
    "    if np.all(pi == pi_next[loop_count]) :\n",
    "        policyStable = True\n",
    "\n",
    "    loop_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果が正しいことを確認しました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態価値の変化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0. 10.  0.  5.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0. -1.]\n",
      " [ 0.  0. -1.  0.  0.]]\n",
      "[[21.97748527 24.41942809 21.97748528 18.45014107 16.60512696]\n",
      " [19.77973674 21.97748528 19.77973675 16.60512696 14.94461426]\n",
      " [17.80176307 19.77973675 17.80176308 14.94461426 13.45015284]\n",
      " [16.02158676 17.80176308 16.02158677 13.45015284  9.80515284]\n",
      " [14.41942809 16.02158677 14.41942809 12.10513755 10.8946238 ]]\n",
      "[[21.97744338 24.4194006  21.97746054 19.4194006  17.47746054]\n",
      " [19.77969904 21.97746054 19.77971449 17.80174304 15.72971449]\n",
      " [17.80172914 19.77971449 17.80174304 16.02156873 14.15674304]\n",
      " [16.02155622 17.80174304 16.02156873 14.41941186 12.74106873]\n",
      " [14.4194006  16.02156873 14.41941186 12.97747068 11.67972361]]\n",
      "[[21.97744338 24.4194006  21.97746054 19.4194006  17.47746054]\n",
      " [19.77969904 21.97746054 19.77971449 17.80174304 16.02156873]\n",
      " [17.80172914 19.77971449 17.80174304 16.02156873 14.41941186]\n",
      " [16.02155622 17.80174304 16.02156873 14.41941186 12.97747068]\n",
      " [14.4194006  16.02156873 14.41941186 12.97747068 11.67972361]]\n"
     ]
    }
   ],
   "source": [
    "print(V_next[0].reshape([5,5]))\n",
    "print(V_next[1].reshape([5,5]))\n",
    "print(V_next[2].reshape([5,5]))\n",
    "print(V_next[3].reshape([5,5]))\n",
    "print(V_next[4].reshape([5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方策の変化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3 3 1]\n",
      " [1 2 0 2 1]\n",
      " [0 3 0 2 3]\n",
      " [3 2 3 0 3]\n",
      " [0 2 1 3 2]]\n",
      "[[3 0 2 0 2]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 2]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 2]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 2 2]\n",
      " [3 0 0 0 2]\n",
      " [3 0 0 0 2]\n",
      " [3 0 0 0 2]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 2 2]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# (0,1,2,3)は(up,down,left,right)\n",
    "\n",
    "print(pi_next[0].reshape([5,5]))\n",
    "print(pi_next[1].reshape([5,5]))\n",
    "print(pi_next[2].reshape([5,5]))\n",
    "print(pi_next[3].reshape([5,5]))\n",
    "print(pi_next[4].reshape([5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c#%E6%96%B9%E7%AD%96%E5%8F%8D%E5%BE%A9)  \n",
    "- [強化学習：再帰処理と反復処理](https://shirakonotempura.hatenablog.com/entry/2019/01/31/070932)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
