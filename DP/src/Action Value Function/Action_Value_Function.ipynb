{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "<span style=\"font-size: 200%;\">行動価値関数 (Action Value Function)</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2019/12/25\n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![問題設定](figure/ProblemSetting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\"> 今回は状態価値関数の時と同様な，5×5の格子世界におけるAgentの行動価値を導出する</span>  \n",
    "- <span style=\"font-size: 120%;\"> ルールは前回と同様なので，[状態価値関数](https://github.com/rrrrind/reinforcement-learning/blob/master/DP/src/State%20Value%20Function/State_Value_Function.ipynb)の問題設定を参照してください</span>  \n",
    "- <span style=\"font-size: 120%;\"> 行動価値は，\n",
    "    $$\n",
    "    {\\rm Q^{\\pi}}(s,a) = \\Sigma_{s'}{\\rm P}^{a}_{s,s'}\\bigr[{\\rm R}^{a}_{s,s'} + \\\n",
    "    \\gamma  \\Sigma_{a}{\\rm \\pi} (s',a') {\\rm Q^{\\pi}}(s',a') \\bigl], \\nonumber\n",
    "    $$\n",
    "によって算出する</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルコフ決定過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> こちらも前回と同じ．[状態価値関数](https://github.com/rrrrind/reinforcement-learning/blob/master/DP/src/State%20Value%20Function/State_Value_Function.ipynb)のマルコフ決定過程を参照．\n",
    "</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonでの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">　エージェントの動作も前回と同じなので，\n",
    "    <span style=\"color: red; \">状態価値関数の部分だけを行動価値関数に書き換える</span>．\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = [[0,4],[1,4],[2,4],[3,4],[4,4],\n",
    "         [0,3],[1,3],[2,3],[3,3],[4,3],\n",
    "         [0,2],[1,2],[2,2],[3,2],[4,2],\n",
    "         [0,1],[1,1],[2,1],[3,1],[4,1],\n",
    "         [0,0],[1,0],[2,0],[3,0],[4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): # stage_map, the array number of the stage    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # 行動a,方策πの宣言。引数で使うので、初めに宣言する。\n",
    "        self.actions = [[0,1],[0,-1],[-1,0],[1,0]] # up,down,left,right\n",
    "        self.each_pi = [0.25,0.25,0.25,0.25]\n",
    "        self.position = []\n",
    "    \n",
    "    def get_actions(self):\n",
    "        # 全行動の取得\n",
    "        return self.actions\n",
    "        \n",
    "    def set_pos(self,now_pos):\n",
    "        # 現在地の更新\n",
    "        self.position = now_pos\n",
    "        \n",
    "    def get_pos(self):\n",
    "        # 現在地の取得\n",
    "        return self.position\n",
    "    \n",
    "    def move(self,action):\n",
    "        # 行動後の位置の取得\n",
    "        if self.get_pos() == [1,4]: # 10ptゾーン\n",
    "            next_pos = [1,0]\n",
    "        elif self.get_pos() == [3,4]: # 5ptゾーン\n",
    "            next_pos = [3,2]\n",
    "        else :\n",
    "            next_pos = [self.get_pos()[0] + action[0], self.get_pos()[1] + action[1]]\n",
    "        \n",
    "        # 境界線外に出ている時の処理\n",
    "        if 0 > next_pos[0] or next_pos[0] > 4 or 0 > next_pos[1] or next_pos[1] > 4:\n",
    "            next_pos = self.get_pos()\n",
    "            \n",
    "        self.set_pos(next_pos)\n",
    "    \n",
    "    def pi(self,state,num):\n",
    "        return self.each_pi[num]\n",
    "        \n",
    "    def reward(self,state,action):\n",
    "        if state == [1,4]:\n",
    "            return 10\n",
    "        \n",
    "        if state == [3,4]:\n",
    "            return 5\n",
    "        \n",
    "        if state[1] == 4 and action == [0,1]:\n",
    "            return -1\n",
    "        elif state[1] == 0 and action == [0,-1]:\n",
    "            return -1\n",
    "        elif state[0] == 0 and action == [-1,0]:\n",
    "            return -1\n",
    "        elif state[0] == 4 and action == [1,0]:\n",
    "            return -1\n",
    "        else :\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "#------------------------------------  以下を行動価値関数に書き換える  ------------------------------------\n",
    "\n",
    "    def q_pi(self,state,act,n,out,iter_num): # この\"state\"は，潜った時の位置を記憶しておくため\n",
    "        if n == iter_num :\n",
    "            for i, action in enumerate(self.actions):\n",
    "                # 最下層での処理．最下層では前後左右に動いた際の報酬を一気に計算する．\n",
    "                out += self.pi(state,i) * self.reward(state,action)\n",
    "            return out\n",
    "        else :\n",
    "            # 行動を1つ選択した後の報酬なのでfor文の外に記述する\n",
    "            # (以下のfor文は4回，つまり上下左右の報酬を求めるためのループ文)\n",
    "            out += self.reward(self.get_pos(),act)\n",
    "            self.move(act)\n",
    "            position = self.get_pos()\n",
    "            for i, action in enumerate(self.actions):\n",
    "            # 行動した後は方策に従う，つまり上下左右の報酬を求める．\n",
    "                out += 0.9 * self.pi(self.get_pos(),i) * self.q_pi(self.get_pos(), action, n+1, 0, iter_num)\n",
    "                self.set_pos(position) # \"self.get_pos()\"から1つ前の状態\"state\"に戻るため\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 60行目の${\\rm \\pi}(s,a)$に$\\gamma$を掛けていないが，これは${\\rm  Q^{\\pi}}(s,a)$によって得られる報酬に対して時間割引$\\gamma$が掛けられていたので，再帰をしない，つまり${\\rm  Q^{\\pi}}(s,a)$がなくなると$\\gamma$も同じくなくなると私は考えている(合ってるかどうかは分からない)．\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb93d8f99cd945d5b7a9edef31684771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elapsed_time:551.5425078868866[sec]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "q_pi_value = np.zeros([25,4])\n",
    "\n",
    "start = time.time()\n",
    "for i,locate in tqdm(enumerate(stage)):\n",
    "    for j, action in enumerate(agent.get_actions()):\n",
    "        agent.set_pos(locate)\n",
    "        q_pi_value[i][j] = agent.q_pi(agent.get_pos(),action,0,0,10)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.00449195,  1.34879984,  2.00449195,  8.01656136],\n",
       "       [ 8.87490074,  8.87490074,  8.87490074,  8.87490074],\n",
       "       [ 2.99431619,  1.99623722,  8.01656136,  4.770906  ],\n",
       "       [ 5.29951596,  5.29951596,  5.29951596,  5.29951596],\n",
       "       [ 0.24642576,  0.41080755,  4.770906  ,  0.24642576],\n",
       "       [ 3.00449195,  0.03079759,  0.34879984,  2.70058495],\n",
       "       [ 8.01656136,  0.64937268,  1.34879984,  1.99623722],\n",
       "       [ 3.99431619,  0.60480828,  2.70058495,  1.66493039],\n",
       "       [ 4.770906  ,  0.29951596,  1.99623722,  0.41080755],\n",
       "       [ 1.24642576, -0.38440394,  1.66493039, -0.58919245],\n",
       "       [ 1.34879984, -0.84691265, -0.96920241,  0.64937268],\n",
       "       [ 2.70058495, -0.34432625,  0.03079759,  0.60480828],\n",
       "       [ 1.99623722, -0.27917008,  0.64937268,  0.29951596],\n",
       "       [ 1.66493039, -0.46541157,  0.60480828, -0.38440394],\n",
       "       [ 0.41080755, -1.00787557,  0.29951596, -1.38440394],\n",
       "       [ 0.03079759, -1.59016659, -1.84691265, -0.34432625],\n",
       "       [ 0.64937268, -1.12509926, -0.84691265, -0.27917008],\n",
       "       [ 0.60480828, -1.00318132, -0.34432625, -0.46541157],\n",
       "       [ 0.29951596, -1.17119903, -0.27917008, -1.00787557],\n",
       "       [-0.38440394, -1.65663417, -0.46541157, -2.00787557],\n",
       "       [-0.84691265, -2.59016659, -2.59016659, -1.12509926],\n",
       "       [-0.34432625, -2.12509926, -1.59016659, -1.00318132],\n",
       "       [-0.27917008, -2.00318132, -1.12509926, -1.17119903],\n",
       "       [-0.46541157, -2.17119903, -1.00318132, -1.65663417],\n",
       "       [-1.00787557, -2.65663417, -1.17119903, -2.65663417]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = np.array(q_pi_value)\n",
    "#outcome = outcome.reshape(5,5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果・考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![結果](figure/outcome.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\"> 4方向への行動確率が同じ( つまり$\\pi(s,a)=0.25$ の)場合における行動価値を算出した </span>\n",
    "- <span style=\"font-size: 120%;\"> 上の図は，再帰を10回行ったときの結果である</span>\n",
    "- <span style=\"font-size: 120%;\"> 結果として，基本『up』の確率を高くすれば，方策がより良くなるということが分かった</span>\n",
    "- <span style=\"font-size: 120%;\"> また再帰を行うと，行動価値を求めるのに莫大な時間が掛かるということも分かった(10分弱)</span>\n",
    "- <span style=\"font-size: 120%;\"> したがって，再帰の代わりに行動価値関数を近似する等して，時間短縮を図る必要があると考えた</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [今さら聞けない強化学習（3）：行動価値関数とBellman方程式](https://qiita.com/triwave33/items/8966890701169f8cad47)\n",
    "- [今さら聞けない強化学習（4）：行動価値関数の実装](https://qiita.com/triwave33/items/669a975b74461559addc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
