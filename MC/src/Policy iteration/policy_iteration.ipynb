{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "<span style=\"font-size: 200%;\">方策反復(Policy iteration)</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/1/10 \n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回は方策反復によって，方策の更新を行う． </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策反復(Policy iteration)とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 方策反復とは，\n",
    "    <span style=\"color: red\"> **反復方策評価(Iterative Policy Evaluation)**</span>\n",
    "    によって算出した状態価値から\n",
    "    <span style=\"color: red\"> **方策改善(Policy improvement)**</span>\n",
    "    により，ある方策下での行動報酬が最大となるように方策を更新していくことである．</span>\n",
    "<span style=\"font-size: 120%;\"> より分かりやすく説明すると， </span> </br>\n",
    "\n",
    "1. <span style=\"font-size: 120%;\"> ある方策$\\pi$が存在するとき </span> </br>\n",
    "1. <span style=\"font-size: 120%;\"> その方策$\\pi$に従ったときの状態価値を算出し(反復方策評価) </span> </br>\n",
    "1. <span style=\"font-size: 120%;\"> その状態時における最適な方策$\\pi^*$を算出する(方策改善) </span> </br>\n",
    "\n",
    "<span style=\"font-size: 120%;\"> ということであり，1～3を繰り返すことを\n",
    "    <span style=\"color: red\">**方策反復**(Policy iteration)</span>\n",
    "    という．</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反復方策評価(Iterative Policy Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> これは，今まで再帰処理によって状態価値を求めていたけど，時間がかかるから反復処理で状態価値関数の近似を試みるってだけな話です．前回実装したんで，気になる方はこちらを参照． </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策改善(Policy improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 今回は割とこれがメインです． </span> </br>\n",
    "<span style=\"font-size: 120%;\"> 初めに，次の図を参照してほしい(最適方策)． </span> </br>\n",
    "<span style=\"font-size: 120%;\"> ※ 方策改善の1つに最適方策がある感じ </span> </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![方策改善](policy_improvement.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> ある状態ss'が与えられた時、取り得る行動（黒丸）が4つに分岐しています。これらのどれが一番良い行動なのかを決めるのがこの章の目的です。</span>\n",
    "\n",
    "<span style=\"font-size: 120%;\"> 行動aをとった結果は、状態遷移確率Pass′Pss′aを経て、報酬Rass′Rss′aおよび次の状態の価値関数Vπ(s)Vπ(s)で表されます。それならば、なるべく多い報酬と次の状態の価値を持つものを最適な行動とするのが自然です。</span>\n",
    "\n",
    "<span style=\"font-size: 120%;\"> すなわち、灰色で囲んだ黒丸以下のユニット∑s′Pass′[Rass′+γVπ(s′)]∑s′Pss′a[Rss′a+γVπ(s′)]が最も高い行動が状態ssでの最適な方策π′(s)π′(s)になります。</span>\n",
    "\n",
    "π′(s)=arg maxa∑s′Pass′[Rass′+γVπ(s′)]\n",
    "π′(s)=arg maxa∑s′Pss′a[Rss′a+γVπ(s′)]\n",
    "<span style=\"font-size: 120%;\"> ただし、arg maxa(x)arg maxa(x)は、xが最大となるようなaを返す関数です。数式よりも図でイメージで理解した方が早いですね。</span>\n",
    "\n",
    "注\n",
    "ここでは、最適な関数を決定論的な関数としています。つまり最適な関数なので、確率的な方策のように「a1a1が90%, a2a2が5%...」とするのではなく、「最適な関数はa1a1」と断言しています。ただし、確率的な方策にも、今回の議論を当てはめることができます。（Sutton本 P.102にその記述があります。）\n",
    "\n",
    "方策改善は，今までのような『upが0.25でdownが0.25で...』という感じではなく，『上下左右の状態価値を見たら，右の価値が一番高いから100%で右行きます!』ってな感じ。だから，一番最初の時点で『状態$s_1$の時は100%下行きます』的なのを決めとかなければならない．最初のはランダムで決めておけば良いかと．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonでの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step1 : 初期化 ----------\n",
    "N = 1000\n",
    "Gamma = 0.9\n",
    "\n",
    "# 格子世界の大きさ\n",
    "x_axis = 5\n",
    "y_axis = 5\n",
    "\n",
    "# 現在の価値関数と更新後の価値関数の初期化\n",
    "V      = np.zeros(x_axis * y_axis)\n",
    "V_next = np.zeros([N, x_axis * y_axis])\n",
    "\n",
    "# 現在の方策と更新後の方策の初期化\n",
    "pi      = np.zeros(x_axis * y_axis, dtype=\"int\")\n",
    "pi_next = np.zeros([N, x_axis * y_axis], dtype=\"int\")\n",
    "\n",
    "# 格子世界の構築\n",
    "stage = [[0,4],[1,4],[2,4],[3,4],[4,4],\n",
    "         [0,3],[1,3],[2,3],[3,3],[4,3],\n",
    "         [0,2],[1,2],[2,2],[3,2],[4,2],\n",
    "         [0,1],[1,1],[2,1],[3,1],[4,1],\n",
    "         [0,0],[1,0],[2,0],[3,0],[4,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの設計\n",
    "# エージェントには，『状態』『行動』『報酬』『方策』を持たせている\n",
    "\n",
    "class Agent():    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # 行動aの宣言。引数で使うので、初めに宣言する。\n",
    "        self.actions = [[0,1],[0,-1],[-1,0],[1,0]] # up,down,left,right\n",
    "    \n",
    "        self.position = []\n",
    "        \n",
    "        # 方策は『各状態時での行動』を持たせている\n",
    "        # 0～3は『up,down,left,right』を示している\n",
    "        self.pi = np.random.randint(0,4,25)\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def set_pos(self,now_pos):\n",
    "        # 現在地の更新\n",
    "        self.position = now_pos\n",
    "        \n",
    "    def get_pos(self):\n",
    "        # 現在地の取得\n",
    "        return self.position\n",
    "    \n",
    "    def set_pi(self,new_pi):\n",
    "        self.pi = new_pi\n",
    "    \n",
    "    def get_pi(self):\n",
    "        return self.pi\n",
    "    \n",
    "    def move(self,action):\n",
    "        # 行動後の位置の取得\n",
    "        if self.get_pos() == [1,4]: # 10ptゾーン\n",
    "            next_pos = [1,0]\n",
    "        elif self.get_pos() == [3,4]: # 5ptゾーン\n",
    "            next_pos = [3,2]\n",
    "        else :\n",
    "            next_pos = [self.get_pos()[0] + action[0], self.get_pos()[1] + action[1]]\n",
    "        \n",
    "        # 境界線外に出ている時の処理\n",
    "        if 0 > next_pos[0] or next_pos[0] > 4 or 0 > next_pos[1] or next_pos[1] > 4:\n",
    "            next_pos = self.get_pos()\n",
    "            \n",
    "        self.set_pos(next_pos)\n",
    "        \n",
    "    def reward(self,state,action):\n",
    "        if state == [1,4]:\n",
    "            return 10\n",
    "        \n",
    "        if state == [3,4]:\n",
    "            return 5\n",
    "        \n",
    "        if state[1] == 4 and action == [0,1]:\n",
    "            return -1\n",
    "        elif state[1] == 0 and action == [0,-1]:\n",
    "            return -1\n",
    "        elif state[0] == 0 and action == [-1,0]:\n",
    "            return -1\n",
    "        elif state[0] == 4 and action == [1,0]:\n",
    "            return -1\n",
    "        else :\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ここから方策反復(Policy iteration) #####\n",
    "#        アルゴリズム通りに実装します       #\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "loop_count = 1\n",
    "pi_next[loop_count-1] = agent.get_pi()\n",
    "\n",
    "policyStable = False\n",
    "while(policyStable == False):\n",
    "    V = np.zeros(x_axis * y_axis)\n",
    "    # ---------- Step2 : 反復方策評価 ----------\n",
    "    while(True):\n",
    "        for i in range(y_axis):\n",
    "            for j in range(x_axis):\n",
    "                pos = i*5+j\n",
    "                delta = 0\n",
    "                agent.set_pos(stage[pos])\n",
    "                v = V[pos]\n",
    "                agent.move(agent.get_actions()[agent.get_pi()[pos]])\n",
    "                V[pos] = (agent.reward(stage[pos], agent.get_actions()[agent.get_pi()[pos]])\n",
    "                       + (Gamma * V[stage.index(agent.get_pos())]))\n",
    "                delta = max(delta,abs(v - V[pos]))\n",
    "        if delta < 1e-5:\n",
    "            break\n",
    "    V_next[loop_count] = V\n",
    "\n",
    "    \n",
    "    # ---------- Step3 : 方策改善 ----------\n",
    "    pi = agent.get_pi()\n",
    "    temp = np.zeros(len(agent.get_actions()))\n",
    "    for a in range(x_axis*y_axis):\n",
    "        for b, action in enumerate(agent.get_actions()):\n",
    "            agent.set_pos(stage[a])\n",
    "            agent.move(action)\n",
    "            temp[b] = (agent.reward(stage[a], action)\n",
    "                    + (Gamma * V[stage.index(agent.get_pos())]))\n",
    "        pi_next[loop_count,a] = np.argmax(temp)\n",
    "    agent.set_pi(pi_next[loop_count])\n",
    "    \n",
    "    if np.all(pi == pi_next[loop_count]) :\n",
    "        policyStable = True\n",
    "\n",
    "    loop_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果が正しいことを確認しました"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態価値の変化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.    10.     0.     5.    -1.   ]\n",
      " [ 0.     0.     0.     4.5    4.05 ]\n",
      " [ 0.     0.     0.     0.     3.645]\n",
      " [ 0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.    -1.    -1.     0.   ]]\n",
      "[[21.97748527 24.41942809 21.97748528 18.45014107 16.60512696]\n",
      " [19.77973674 21.97748528 14.94460119 16.60512696 14.94461426]\n",
      " [17.80176307 19.77973675 13.45014107 14.94461426 13.45015284]\n",
      " [16.02158676 17.80176308 12.10512696 13.45015284 12.10513755]\n",
      " [14.41942809 16.02158677 10.89461426 12.10513755 10.8946238 ]]\n",
      "[[21.97748527 24.41942809 21.97748528 18.45014107 16.60512696]\n",
      " [19.77973674 21.97748528 19.77973675 16.60512696 14.94461426]\n",
      " [17.80176307 19.77973675 17.80176308 14.94461426 13.45015284]\n",
      " [16.02158676 17.80176308 16.02158677 13.45015284 12.10513755]\n",
      " [14.41942809 16.02158677 14.41942809 12.10513755 10.8946238 ]]\n",
      "[[21.97746054 24.41941186 21.97747068 19.41941186 17.47747068]\n",
      " [19.77971449 21.97747068 19.77972361 17.80175125 15.72972361]\n",
      " [17.80174304 19.77972361 17.80175125 16.02157612 14.15675125]\n",
      " [16.02156873 17.80175125 16.02157612 14.41941851 12.74107612]\n",
      " [14.41941186 16.02157612 14.41941851 12.97747666 11.46696851]]\n"
     ]
    }
   ],
   "source": [
    "print(V_next[0].reshape([5,5]))\n",
    "print(V_next[1].reshape([5,5]))\n",
    "print(V_next[2].reshape([5,5]))\n",
    "print(V_next[3].reshape([5,5]))\n",
    "print(V_next[4].reshape([5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方策の変化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 3 0 3]\n",
      " [1 1 2 0 2]\n",
      " [0 1 2 2 0]\n",
      " [3 1 3 1 1]\n",
      " [0 0 1 1 0]]\n",
      "[[3 0 2 0 2]\n",
      " [0 0 3 0 2]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 0 0]\n",
      " [3 0 2 0 0]\n",
      " [3 0 2 0 0]\n",
      " [3 0 2 0 0]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 0]\n",
      " [3 0 0 2 0]]\n",
      "[[3 0 2 0 2]\n",
      " [3 0 0 2 2]\n",
      " [3 0 0 0 2]\n",
      " [3 0 0 0 2]\n",
      " [3 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# (0,1,2,3)は(up,down,left,right)\n",
    "\n",
    "print(pi_next[0].reshape([5,5]))\n",
    "print(pi_next[1].reshape([5,5]))\n",
    "print(pi_next[2].reshape([5,5]))\n",
    "print(pi_next[3].reshape([5,5]))\n",
    "print(pi_next[4].reshape([5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c#%E6%96%B9%E7%AD%96%E5%8F%8D%E5%BE%A9) </br>\n",
    "[強化学習：再帰処理と反復処理](https://shirakonotempura.hatenablog.com/entry/2019/01/31/070932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
