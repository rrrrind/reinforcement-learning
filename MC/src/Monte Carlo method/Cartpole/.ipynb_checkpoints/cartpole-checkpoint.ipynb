{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 200%;\">モンテカルロ法 (Monte Carlo method)</span>\n",
    "</div><br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 180%;\">～OpenAI GymのCartpole編～</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/1/22\n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> モンテカルロ法による，Cartpoleの方策改善を行う．</span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](figure/Cartpole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Cartpoleとは，棒の乗った台車を左右に動かすことで，棒を倒さないように台車の動きを制御するゲームである．この台車を強化学習により制御するためには，状態$s$，行動$a$，方策$\\pi$，報酬$r$を考えなければならない(マルコフ決定過程)．まず状態$s$についてだが，これは『台車の位置』，『台車の速度』，『棒の傾き(角度)』，『棒の角速度(倒れるときのスピード)』が考えられる．次に行動$a$についてだが，これは『台車を右へ動かす動作』と『台車を左へ動かす動作』の2つが考えられる．方策は$\\pi(s,a)$は，初めはどんな状態に対してもランダムな行動を取ることとする．最後に報酬$r$についてだが，モンテカルロ法は終了地点が存在しなければならないため，ステップ数を200までとし，1ステップ進むことに1ptの報酬を与えることとする．まとめると，</span> \\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\">状態s&isin;S：\"台車の位置\"，\"台車の速度\"，\"棒の傾き(角度)\"，\"棒の角速度(倒れるときのスピード)\"</span> \\\n",
    "\n",
    "<span style=\"font-size: 120%;\">行動a&isin;A：\"台車を右へ動かす動作\" or \"台車を左へ動かす動作\"</span> \\\n",
    "\n",
    "<span style=\"font-size: 120%;\">報酬R(s,a,s')： </span>\n",
    "    $$\n",
    "    {\\rm R(s,a,s')} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    1pt & (\\mbox{1ステップ進む毎に}) \\\\\n",
    "    0pt & (\\mbox{棒が倒れた場合})\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$ \\\n",
    "<span style=\"font-size: 120%;\">方策$\\pi$(s,a)： </span>\n",
    "    $$\n",
    "    {\\rm \\pi(s,a)} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    0.5 & (any, right) \\\\\n",
    "    0.5 & (any, left)\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$ \\\n",
    "\n",
    "<span style=\"font-size: 120%;\"> となる．何故，行動価値関数を用いて方策評価/改善を行うのかは以下の<u>**Q&A**</u>を参照してほしい．</span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装までの流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行動価値関数を用いた方策評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> ~~これはステップ数(今回は200)を決定し，決められたサンプリング回数までエージェントを動かすことにより，行動価値を求めることができる．状態価値と異なる点として，状態$s_1$から始めるか，状態$s_{1}$の時に行動$a_{1}$をしてから始めるかが挙げられる．つまり，状態価値は得られる値が1つなのに対し，行動価値は4つの値を得ることができる．~~ </span> \\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\"> どうやら動的計画法とモンテカルロ法では，行動価値を求める方法が異なるらしい．動的計画法では，状態価値を求め， </span> \\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\"> \n",
    "    $$\n",
    "    {\\rm Q^{\\pi}}(s,a) = \\Sigma_{s'}{\\rm P}^{a}_{s,s'}\\bigr[{\\rm R}^{a}_{s,s'} + \\\n",
    "    \\gamma \\mathrm{V^{\\pi}}(s) \\bigr], \n",
    "    $$\n",
    "</span> \\\n",
    "    <span style=\"font-size: 120%;\"> のベルマン方程式を用いることで行動価値を算出することができた($\\rm{Q}$ to $\\rm{Q}$の式も同じ)．しかしモンテカルロ法では${\\rm P}^{a}_{s,s'}$を使うことができない(わからない)ので，従来の『行動を1つ選択してから求めた状態価値が行動価値』という方法を使うことができない．また今回のCartpole問題は，スタート地点は必ず[0,0,0,0]の状態(つまり台車が中心にあり，棒が台車に対して垂直な状態)となるので，『全ての状態を開始点にとることができる』と『経験しない枝分かれ状態があってはならない(開始点探査)』の問題に対して少し工夫をしなければならない(なぜなら，位置が左寄りで棒が少し傾いた状態でスタートなどができず，また，ある状態の時は必ず右に行くという方策が取られた場合，経験しない枝分かれができてしまう)．『経験しない枝分かれ状態があってはならない』に対しては$\\epsilon$ - greedy法で解決できる(以下に詳細有)．『全ての状態を開始点にとることができる』に関しては，スタート地点を変えることはできないので，スタートから終点までの状態と行動，また報酬を全て保存することで，まるでスタート地点が変わったかのようにみなすこととする．とりあえず，次の図を見てほしい．</span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/active_value.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> この図では，$s_t$がスタート地点，$s_{t+3}$が終了地点となっており，終点地点までの行動は$a_t=$左，$a_{t+1}=$右，$a_{t+2}=$右，となっている．この時，スタート地点$s_t$で行動$a_t$を取った時の行動価値は， </span>\\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\"> \n",
    "    $$\n",
    "    {\\rm Q^{\\pi}}_t(s_t,a_{t}=左) = r_{t+1}+\\gamma[r_{t+2}+\\gamma[r_{t+3}]], \n",
    "    $$\n",
    "</span> \\\n",
    "<span style=\"font-size: 120%;\"> となる(多分あってるはず)．本来であればここで次のエピソードに進むが，仮に$s_{t+1}$をスタート地点とみなすと，$s_{t+1}$から$s_{t+3}$までを1つのエピソードとしてみなすことができる．つまり， </span>\\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\"> \n",
    "    $$\n",
    "    {\\rm Q^{\\pi}}_{t+1}(s_{t+1},a_{t+1}=右) = r_{t+2}+\\gamma[r_{t+3}], \n",
    "    $$\n",
    "</span> \\\n",
    "<span style=\"font-size: 120%;\"> となり，$s_{t+1}$をスタート地点としたときに行動$a_{t+1}$を行ったときの行動価値を導出することができる．</span>\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行動価値関数を用いた方策改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 前前回の『方策反復(Policy iteration)』では，状態価値を求めた後に行動価値を求め方策改善を行うという二度手間を踏んでいたが，今回は直接行動価値を求め，方策改善を行うこととする．なぜなら，今回は状態価値を知っても意味がないからである(確認したいのは棒の動き)．この辺は動的計画法と同じである．</span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ - greedy法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\">モンテカルロ法は，『経験しない枝分かれ状態があってはならない（全ての状態を無限回訪問する）』という決まり(開始点探査)があるが，仮に状態$s_n$の時は必ず右に行くというようにと学習された場合，それ以降，状態$s_n$の時に左へ移動するという経験を得ることができなくなるため，正しく方策を更新できなくなるという問題がある．そこで$\\epsilon$-greedy法を用いることでこの問題を解決する．説明は[ここ](https://www.tcom242242.net/entry/2017/01/15/163250/)に書いてあるんで，各自読んどいてください．すごく簡単に説明すると，たまには方策以外の行動も取ろう!的な感じです．数式は，</span> \\\n",
    "<br>\n",
    "<span style=\"font-size: 120%;\">\n",
    "$$\n",
    "\\pi(s,a,s') = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    1-\\epsilon+\\frac{\\epsilon}{|A(s)|} & (a = a^{*}) \\\\\n",
    "    \\frac{\\epsilon}{|A(s)|} & (a \\neq a^{*})\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$ \n",
    "<br>\n",
    "で表される．この時$\\epsilon$は定数(基本0.3)であり，$|A(s)|$は行動$a$の総数である．</span> \\\n",
    "<span style=\"font-size: 110%;\"> ※Grid Worldを例に挙げると，$|A(s)|$は4なので，最も良いとされる行動は$1-0.3+\\frac{0.3}{4}=0.7749...\\simeq0.775$の確率で実行されることとなり，その他の行動は$\\frac{0.3}{4}=0.075$の確率で実行される．総和を取ると，$0.775+0.075+0.075+0.075=0.999...\\simeq1$となる．</span> \\\n",
    "<span style=\"font-size: 110%;\"> ※$\\epsilon$の値が大きすぎると行動がランダムに近くなるため報酬が不安定になり，逆に小さすぎると決まった行動ばかり行うので，最適な方策を探索するのに時間がかかる．</span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "step_num = 300     # 何ステップまで実行するか\n",
    "episode_num = 2001 # 何エピソード数実行するか(動画を保存するため+1してる)\n",
    "\n",
    "max_list = [0.15, 0.8, 0.025, 0.5] # 状態の最大値\n",
    "min_list = [-0.2, -2, -0.05, -1] # 状態の最小値\n",
    "division = 5 # 状態の分割数\n",
    "\n",
    "action = [0,1]\n",
    "actions = np.random.randint(0,2,division**len(max_list)) # 左 or 右 が 1296個\n",
    "policy = np.ones([division**len(max_list),len(action)]) * (1/len(action)) # 方策[0.5,0.5]が1296個\n",
    "episode = np.zeros(step_num,np) # 方策によって決められた1エピソード分の行動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントクラス(Agent Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> エージェントクラスでは，．</span> \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,actions,policy,episode):\n",
    "        self.state = [] # 1エピソード分の状態配列，どこまで進んだか．\n",
    "        self.actions = actions # 各状態における行動(今回は1296個)\n",
    "        self.policy = policy # 方策[0.5,0.5]が全ての状態(今回は[1296,2])個\n",
    "        self.reward = [] # 1エピソード分の報酬\n",
    "        self.episode = episode # 方策によって決められた，各状態での行動(今回は100個)\n",
    "        \n",
    "    def set_state(self,state):\n",
    "        self.state = state # 現在の状態の更新\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def set_actions(self,actions):\n",
    "        self.actions = actions\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def set_policy(self,policy):\n",
    "        self.policy = policy \n",
    "    \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "    \n",
    "    def set_reward(self,reward):\n",
    "        self.reward = reward\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    def set_episode(self,episode):\n",
    "        self.episode = episode\n",
    "    \n",
    "    def get_episode(self):\n",
    "        return self.episode    \n",
    "    \n",
    "    def e_greedy(self,e=0.3): # 方策をすべてe-greedyする\n",
    "        policy = np.zeros(np.shape(self.get_policy()))\n",
    "        main_a = 1 - e + (e / len(self.get_policy()[0]))\n",
    "        other_a = e / len(self.get_policy()[0])\n",
    "        for i, action in enumerate(self.get_actions()):\n",
    "            policy[i] = [main_a,other_a] if action == 0 else [other_a,main_a]\n",
    "        self.set_policy(policy)\n",
    "        \n",
    "    def discretize(self,state, max_list, min_list, division):\n",
    "        disc_s = np.zeros(len(state),dtype=np.int32)\n",
    "        disc_s = [np.digitize(state[i], np.linspace(min_list[i], max_list[i], division-1)) for i in range(len(state))]\n",
    "        return disc_s\n",
    "        \n",
    "    def move(self,max_list,min_list,division,step_num): # moveで1エピソード分実行し，その時の状態と報酬を保存する\n",
    "        observation = env.reset() # cartpole環境の初期化\n",
    "        state_array = np.zeros([step_num+1,len(observation)])\n",
    "        state_array[0] = self.discretize(observation,max_list,min_list,division) # 現在の状態を離散化\n",
    "        reward_array = np.zeros(step_num+1)\n",
    "        episode_array = np.zeros(step_num+1,dtype=np.int32) # 1エピソードの行動を記憶する\n",
    "        for i in range(step_num):\n",
    "            # 現在の状態の位置\n",
    "            position = np.int(np.sum([state_array[i][j] * division ** j for j in range(len(state_array[i]))]))\n",
    "            # その時の方策から行動を決定\n",
    "            episode_array[i] = random.choices([0,1], k=1, weights=self.get_policy()[position])[0] \n",
    "            # 実行\n",
    "            observation, reward, done, info = env.step(episode_array[i])\n",
    "            # 現在の状態を離散化\n",
    "            state_array[i+1] = self.discretize(observation,max_list,min_list,division)\n",
    "            # 報酬の記憶\n",
    "            reward_array[i] = reward\n",
    "            if done == True:\n",
    "                break\n",
    "        self.set_state(state_array)\n",
    "        self.set_reward(reward_array)\n",
    "        self.set_episode(episode_array)\n",
    "        return i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境クラス(Environment Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 環境クラスでは，エージェントクラスの呼び出し，エピソードの実行，結果の離散化，行動価値の算出，方策改善を行う．</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, actions, policy, episode, episode_num, step_num):\n",
    "        self.step_num = step_num\n",
    "        self.episode_num = episode_num\n",
    "        self.active_value = []\n",
    "        self.active_value_temp = []\n",
    "        self.active_value_count = []\n",
    "        \n",
    "        self.agent = Agent(actions,policy,episode)\n",
    "        \n",
    "    def set_active_value(self,active_value):\n",
    "        self.active_value = active_value\n",
    "        \n",
    "    def get_active_value(self):\n",
    "        return self.active_value\n",
    "        \n",
    "    def set_active_value_temp(self,active_value_temp):\n",
    "        self.active_value_temp = active_value_temp\n",
    "        \n",
    "    def get_active_value_temp(self):\n",
    "        return self.active_value_temp\n",
    "    \n",
    "    def set_active_value_count(self,active_value_count):\n",
    "        self.active_value_count = active_value_count\n",
    "        \n",
    "    def get_active_value_count(self):\n",
    "        return self.active_value_count\n",
    "    \n",
    "    def active_value_function(self,states,rewards,episode,div,count):\n",
    "        active_value = self.get_active_value() # 現在の行動価値の取得\n",
    "        active_value_temp = self.get_active_value_temp() # 平均を取るための一時的な箱\n",
    "        active_value_count = self.get_active_value_count() # 現在までのサンプリング数\n",
    "        temp_value = 0\n",
    "        true_state = states[:count]\n",
    "        for i,state in enumerate(reversed(true_state)): # 後ろから計算することで計算コストを抑える．式(2)を見て．\n",
    "            pos = np.int(np.sum([state[j] * div ** j for j in range(len(state))])) # ここで0~1295のどの状態かを特定する．    \n",
    "            # ----- ここから行動価値を算出する -----\n",
    "            temp = len(true_state)-(i+1) # 配列の後ろから処理を行う\n",
    "            temp_value = rewards[temp] + 0.9 * temp_value # 行動価値を代入\n",
    "            active_value_temp[pos,episode[temp],active_value_count[pos,episode[temp]]] = temp_value # 行動価値の可算            \n",
    "            active_value_count[pos,episode[temp]] += 1 # 行動価値の可算回数                \n",
    "            active_value[pos,episode[temp]] = np.sum(active_value_temp[pos,episode[temp]])/ \\\n",
    "                                              active_value_count[pos,episode[temp]]\n",
    "            # --------------------------------------\n",
    "            \n",
    "        self.set_active_value(active_value)\n",
    "        self.set_active_value_temp(active_value_temp)\n",
    "        self.set_active_value_count(active_value_count)\n",
    "        \n",
    "    def policy_improvement(self,loop):\n",
    "        active_values = self.get_active_value()\n",
    "        actions = self.agent.get_actions()\n",
    "        for i,active_value in enumerate(active_values):\n",
    "            actions[i] = 0 if active_value[0] > active_value[1] else 1        \n",
    "        self.agent.set_actions(actions)\n",
    "        self.agent.e_greedy(e = 0.3 if loop < 1500 else 0.1 )\n",
    "\n",
    "    def run(self, max_list, min_list, division):\n",
    "        count = np.zeros(self.episode_num, dtype=np.int32)\n",
    "        rewards_array = np.zeros(self.episode_num, dtype=np.int32)\n",
    "        \n",
    "        num_action = len(self.agent.get_policy()[0]) # 行動の数．2\n",
    "        self.set_active_value(np.zeros([division**len(max_list),num_action])) # [1296,2]\n",
    "        self.set_active_value_temp(np.zeros(\n",
    "            [division**len(max_list),num_action,self.episode_num*self.step_num])) # [1296,2,episode_num*step_num]\n",
    "        self.set_active_value_count(np.zeros([division**len(max_list),num_action],dtype=np.int32)) # [1296,2]\n",
    "        for i in range(self.episode_num):\n",
    "            count[i] = self.agent.move(max_list,min_list,division,self.step_num)\n",
    "            states = self.agent.get_state()\n",
    "            rewards = self.agent.get_reward()\n",
    "            self.active_value_function(states,rewards,self.agent.get_episode(),division,count[i])\n",
    "            self.policy_improvement(i)\n",
    "            rewards_array[i] = np.sum(rewards)\n",
    "        env.close()\n",
    "        return self.agent.get_policy(), rewards_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メイン部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2beb13b70eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moptim_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb4f1e8cc583>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, max_list, min_list, division)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_active_value_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdivision\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [1296,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-871f394305e4>\u001b[0m in \u001b[0;36mmove\u001b[0;34m(self, max_list, min_list, division, step_num)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# moveで1エピソード分実行し，その時の状態と報酬を保存する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cartpole環境の初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mstate_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstate_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdivision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 現在の状態を離散化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;31m# Set these in reverse order to above, to ensure we get user preference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mcreate_context\u001b[0;34m(self, share)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXlibContextARB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mXlibContext13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mXlibContext13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseXlibContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibContext13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# TODO: Check Xlib error generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContextException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not create GL context'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_have_SGI_video_sync\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLX_SGI_video_sync'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextException\u001b[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "save = os.getcwd() + \"\\\\figure\"\n",
    "\n",
    "env = gym.wrappers.Monitor(env, save, video_callable=(lambda ep: ep % 400 == 0), force = True)\n",
    "\n",
    "cartpole = Environment(actions, policy, episode, episode_num,step_num)\n",
    "\n",
    "t1 = time.time()\n",
    "optim_policy, optim_rewards = cartpole.run(max_list, min_list, division)\n",
    "t2 = time.time()\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}s\")\n",
    "\n",
    "with open(\"optim_policy.pickle\", 'wb') as rl:\n",
    "    pickle.dump(optim_policy , rl)\n",
    "    \n",
    "with open(\"optim_rewards.pickle\", 'wb') as rl:\n",
    "    pickle.dump(optim_rewards , rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"optim_policy.pickle\", 'rb') as rl:\n",
    "    optim_policy = pickle.load(rl)\n",
    "with open(\"optim_rewards.pickle\", 'rb') as rl:\n",
    "    optim_rewards = pickle.load(rl)\n",
    "\n",
    "def discretize(state, max_list, min_list, division):\n",
    "    disc_s = np.zeros(len(state),dtype=np.int32)\n",
    "    disc_s = [np.digitize(state[i], np.linspace(min_list[i], max_list[i], division-1)) for i in range(len(state))]\n",
    "    return disc_s\n",
    "    \n",
    "def show_cartpole(policy,max_list,min_list,division,step_num):\n",
    "    observation = env.reset() # cartpole環境の初期化\n",
    "    for i in range(step_num):\n",
    "        env.render()\n",
    "        # 現在の状態を離散化\n",
    "        observation = discretize(observation,max_list,min_list,division)\n",
    "        # 現在の状態の位置\n",
    "        position = np.int(np.sum([observation[j] * division ** j for j in range(len(observation))]))\n",
    "        # 実行\n",
    "        action = 0 if policy[position][0] > policy[position][1] else 1 \n",
    "        observation, reward, done, info = env.step(action)\n",
    "    observation = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期状態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cartpole(policy,max_list,min_list,division,step_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改善後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cartpole(optim_policy,max_list,min_list,division,step_num)\n",
    "\n",
    "#plt.xlabel('episode')\n",
    "#plt.ylabel('return')\n",
    "#plt.plot(optim_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エピソードは多分，その都度状態とその状態時における方策によって決定されると思う．したがって，方策は[1296,2]のサイズが正しく，得られた状態はその場で離散化できるようにしなければいけないと考えられる．\n",
    "状態得る→離散化→その時の方策→行動→終点まで繰り返す→行動価値→方策改善→...だと思う．\n",
    "したがって，エピソードを予め作る必要はない．その都度つくりその都度cartpoleの関数に行動としてぶち込む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> アルゴリズム的に正しく評価できているかわからない(自信がない)が，とりあえず棒を倒さないようにすることはできた．詳細は本を読んで確認しよう! </span> \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Q1, なんで方策$\\pi$の更新に状態価値関数じゃなくて行動価値関数を使うの?</span> \\\n",
    "<span style=\"font-size: 120%;\"> A1, ~~正確には分からないが，今回のcartpoleの問題を考えると，重要なのは状態ではなくて行動ということがわかる．例えば，ポールの傾きが～，角速度が～，...の時の状態は〇〇ということよりも，土台を右にずらした時〇〇，左にずらした時〇〇ということが分かった方が制御がしやすいし考えやすくもある．また，何を更新すればよいかということも分かりやすい．状態だと，傾きを最適にしたいの?角速度を最適にしたいの?ってなる．今まではgrid worldを相手に考えていたので，行動よりも状態を重視しているところがあった．したがって，どのような問題かによって，状態価値関数と行動価値関数を使い分ける必要がある．~~</span> \\\n",
    "<span style=\"font-size: 120%;\"> ~~※遷移確率が使えないからという説明記事があったが，状態価値も行動価値も遷移確率を使わざる負えないので(式的に)，やはり問題によって使い分けるが適切だと考えている．モンテカルロ法は，遷移確率の代わりにサンプリングを用いている．多分...~~</span> \\\n",
    "\n",
    "<span style=\"font-size: 120%;\"> A1, [最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c)にある通り，方策を改善するということは，状態価値がよりよくなるための<span style=\"color: red; \">行動$a$を選択する</span>ということ，つまり報酬が最大となるような行動$a$を選択すればよいということが分かる．そのためには，どの行動$a$を選択した場合に最も報酬が貰えるかを導出する必要がある．これは式で表すと，</span>\\\n",
    "<br>\n",
    "<span style=\"font-size: 130%;\">\n",
    "\\begin{eqnarray}\n",
    "    \\pi^{*}(s,a) &=& \\mathrm{argmax}_{a} \\Sigma_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma \\mathrm{V^{\\pi}}(s')]， \\\\\n",
    "                 &=& \\mathrm{argmax}_{a} \\mathrm{Q^{\\pi}}(s,a)\n",
    "\\end{eqnarray}\n",
    "</span> \\\n",
    "<span style=\"font-size: 120%;\"> となり，最も行動価値が大きくなる行動$a$を選択すればよいことがわかる．動的計画法(Dynamic Programming)では，このQ関数を計算により算出していたが，モンテカルロ法ではQ関数をサンプリングにより導出する．正しくは，$P^{a}_{ss'}$をサンプリングにより補う．Q関数の式はモンテカルロ法でも使うからね!</span>\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/policy_improvement.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[今さら聞けない強化学習(8): モンテカルロ法でOpenAI GymのCartpoleを学習](https://qiita.com/triwave33/items/1b9c87089b2fce0dd481) \\\n",
    "[今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c) \\\n",
    "[ε-greedy行動選択](https://www.tcom242242.net/entry/2017/01/15/163250/) \\\n",
    "[強化学習について学んでみた。（その7）](https://yamaimo.hatenablog.jp/entry/2015/08/23/200000) ← epsilon-greedy法の数式 \\\n",
    "[Introducing CartPole-v1](https://subscription.packtpub.com/book/data/9781789345803/6/ch06lvl1sec47/introducing-cartpole-v1) ← Cartpoleの画像 \\\n",
    "[OpenAI Gym](https://gym.openai.com/envs/#classic_control)\n",
    "[第10回　CartPole課題をQ学習で制御する](https://book.mynavi.jp/manatee/detail/id=88997) ← 分かりやすい実装 \\\n",
    "[【強化学習初心者向け】シンプルな実装例で学ぶSARSA法およびモンテカルロ法【CartPoleで棒立て：1ファイルで完結】](https://qiita.com/sugulu/items/7a14117bbd3d926eb1f2#%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92) ← もっと分かりやすい実装 \\\n",
    "[強化学習について学んでみた。（その14）](https://yamaimo.hatenablog.jp/entry/2015/09/30/200000) ← モンテカルロ法での行動価値の求め方 \\\n",
    "[【強化学習】まとめてみた　第五回（１）（Q学習だけじゃない！モンテカルロ法）【ブラックジャック攻略】](https://qiita.com/MENDY/items/4a896b9800775ad987a7) ← モンテカルロ法が分かりやすい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
