{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 200%;\">モンテカルロ法 (Monte Carlo method)</span>\n",
    "</div><br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 180%;\">～OpenAI GymのCartpole編～</span>\n",
    "</div>\n",
    "</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>\n",
    "<div style=\"text-align: right;\">\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        2020/1/22\n",
    "    </span><br>\n",
    "    <span style=\"font-size: 150%;\">\n",
    "        Masaya Mori\n",
    "    </span>\n",
    "</div>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> モンテカルロ法による，Cartpoleの方策改善を行う．</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](figure/Cartpole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> Cartpoleとは，棒の乗った台車を左右に動かすことで，棒を倒さないように台車の動きを制御するゲームである．この台車を強化学習により制御するためには，状態$s$，行動$a$，方策$\\pi$，報酬$r$を考えなければならない(マルコフ決定過程)．まず状態$s$についてだが，これは『台車の位置』，『台車の速度』，『棒の傾き(角度)』，『棒の角速度(倒れるときのスピード)』が考えられる．次に行動$a$についてだが，これは『台車を右へ動かす動作』と『台車を左へ動かす動作』の2つが考えられる．方策は$\\pi(s,a)$は，初めはどんな状態に対してもランダムな行動を取ることとする．最後に報酬$r$についてだが，モンテカルロ法は終了地点が存在しなければならないため，ステップ数を200までとし，1ステップ進むことに1ptの報酬を与えることとする．まとめると，</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 120%;\">状態s&isin;S：\"台車の位置\"，\"台車の速度\"，\"棒の傾き(角度)\"，\"棒の角速度(倒れるときのスピード)\"</span>\n",
    "- <span style=\"font-size: 120%;\">行動a&isin;A：\"台車を右へ動かす動作\" or \"台車を左へ動かす動作\"</span>\n",
    "- <span style=\"font-size: 120%;\">報酬R(s,a,s')： </span>\n",
    "    $$\n",
    "    {\\rm R(s,a,s')} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    1pt & (\\mbox{1ステップ進む毎に}) \\\\\n",
    "    0pt & (\\mbox{棒が倒れた場合})\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$  \n",
    "- <span style=\"font-size: 120%;\">方策$\\pi$(s,a)： </span>\n",
    "    $$\n",
    "    {\\rm \\pi(s,a)} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    0.5 & (any, right) \\\\\n",
    "    0.5 & (any, left)\n",
    "    \\end{array} \\nonumber\n",
    "    \\right.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> となる．何故，行動価値関数を用いて方策評価/改善を行うのかは以下の<u>**Q&A**</u>を参照してほしい．</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装までの流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動価値関数を用いた方策評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~<span style=\"font-size: 120%;\"> これはステップ数(今回は200)を決定し，決められたサンプリング回数までエージェントを動かすことにより，行動価値を求めることができる．状態価値と異なる点として，状態$s_1$から始めるか，状態$s_{1}$の時に行動$a_{1}$をしてから始めるかが挙げられる．つまり，状態価値は得られる値が1つなのに対し，行動価値は4つの値を得ることができる．</span>~~  \n",
    "\n",
    "<span style=\"font-size: 120%;\"> どうやら動的計画法とモンテカルロ法では，行動価値を求める方法が異なるらしい．動的計画法では状態価値を求め，ベルマン方程式 </span>  \n",
    "\n",
    "$$\n",
    "  {\\rm Q^{\\pi}}(s,a) = \\Sigma_{s'}{\\rm P}^{a}_{s,s'}\\bigr[{\\rm R}^{a}_{s,s'} + \\\n",
    "  \\gamma \\mathrm{V^{\\pi}}(s) \\bigr], \n",
    "$$  \n",
    "\n",
    "<span style=\"font-size: 120%;\"> を用いることで行動価値を算出することができた(from $\\rm{Q}$ to $\\rm{Q}$の式も同じ)．しかし，モンテカルロ法では${\\rm P}^{a}_{s,s'}$を使うことができない(わからない)ので，従来の『行動を1つ選択してから求めた状態価値が行動価値』という方法を使うことができない．また今回のCartpole問題は，スタート地点は必ず[0,0,0,0]の状態(つまり台車が中心にあり，棒が台車に対して垂直な状態)となるので，『全ての状態を開始点にとることができる』と『経験しない枝分かれ状態があってはならない(開始点探査)』の問題に対して少し工夫をしなければならない(なぜなら，位置が左寄りで棒が少し傾いた状態でスタートなどができず，また，ある状態の時は必ず右に行くという方策が取られた場合，経験しない枝分かれができてしまう)．『経験しない枝分かれ状態があってはならない』に対しては$\\epsilon$ - greedy法で解決できる(以下に詳細有)．『全ての状態を開始点にとることができる』に関しては，スタート地点を変えることはできないので，スタートから終点までの状態と行動，また報酬を全て保存することで，スタート地点が変わったかのようにみなすこととする．詳細は次の図に示す．</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/active_value.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> この図では，$s_t$がスタート地点，$s_{t+3}$が終了地点となっており，終点地点までの行動は$a_t=$左，$a_{t+1}=$右，$a_{t+2}=$右，となっている．この時，スタート地点$s_t$で行動$a_t$を取った時の行動価値は，</span>  \n",
    "\n",
    "$$\n",
    "  {\\rm Q^{\\pi}}_t(s_t,a_{t}=左) = r_{t+1}+\\gamma[r_{t+2}+\\gamma[r_{t+3}]], \n",
    "$$  \n",
    "\n",
    "<span style=\"font-size: 120%;\"> となる(多分あってるはず)．本来であればここで次のエピソードに進むが，仮に$s_{t+1}$をスタート地点とみなすと，$s_{t+1}$から$s_{t+3}$までを1つのエピソードとしてみなすことができる．つまり，</span>  \n",
    "\n",
    "$$\n",
    "  {\\rm Q^{\\pi}}_{t+1}(s_{t+1},a_{t+1}=右) = r_{t+2}+\\gamma[r_{t+3}], \n",
    "$$  \n",
    "\n",
    "<span style=\"font-size: 120%;\"> となり，$s_{t+1}$をスタート地点としたときに行動$a_{t+1}$を行ったときの行動価値を導出することができる．</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動価値関数を用いた方策改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 前前回の『[方策反復(Policy iteration)](https://github.com/rrrrind/reinforcement-learning/blob/master/DP/src/Policy%20iteration/policy_iteration.ipynb)』では，状態価値を求めた後に行動価値を求め方策改善を行うという二度手間を踏んでいたが，今回は直接行動価値を求め，方策改善を行うこととする．なぜなら，今回は状態価値を知っても意味がないからである(確認したいのは棒の動き)．この辺は動的計画法と同じである．</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$ - greedy法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> モンテカルロ法は，『経験しない枝分かれ状態があってはならない（全ての状態を無限回訪問する）』という決まり(開始点探査)があるが，仮に状態$s_n$の時は必ず右に行くというようにと学習された場合，それ以降，状態$s_n$の時に左へ移動するという経験を得ることができなくなるため，正しく方策を更新できなくなるという問題がある．そこで$\\epsilon$-greedy法を用いることでこの問題を解決する．詳細は[こちら](https://www.tcom242242.net/entry/2017/01/15/163250/)に書いてあります．すごく簡単に説明すると，たまには方策以外の行動も取ろう!的な感じです．数式は，</span>  \n",
    "\n",
    "$$\n",
    "\\pi(s,a,s') = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    1-\\epsilon+\\frac{\\epsilon}{|A(s)|} & (a = a^{*}) \\\\\n",
    "    \\frac{\\epsilon}{|A(s)|} & (a \\neq a^{*})\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$ \n",
    "\n",
    "<span style=\"font-size: 120%;\"> で表される．この時$\\epsilon$は定数(基本0.3)であり，$|A(s)|$は行動$a$の総数である．</span>  \n",
    "\n",
    "<span style=\"font-size: 110%;\"> ※ Grid Worldを例に挙げると，$|A(s)|$は4なので，最も良いとされる行動は$1-0.3+\\frac{0.3}{4}=0.7749...\\simeq0.775$の確率で実行されることとなり，その他の行動は$\\frac{0.3}{4}=0.075$の確率で実行される．総和を取ると，$0.775+0.075+0.075+0.075=0.999...\\simeq1$となる．</span>  \n",
    "<span style=\"font-size: 110%;\"> ※ $\\epsilon$の値が大きすぎると行動がランダムに近くなるため報酬が不安定になり，逆に小さすぎると決まった行動ばかり行うので，最適な方策を探索するのに時間がかかる．</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "step_num = 300     # 何ステップまで実行するか\n",
    "episode_num = 2001 # 何エピソード数実行するか(動画を保存するため+1してる)\n",
    "\n",
    "max_list = [0.15, 0.8, 0.025, 0.5] # 状態の最大値\n",
    "min_list = [-0.2, -2, -0.05, -1] # 状態の最小値\n",
    "division = 5 # 状態の分割数\n",
    "\n",
    "action = [0,1]\n",
    "actions = np.random.randint(0,2,division**len(max_list)) # 左 or 右 が 1296個\n",
    "policy = np.ones([division**len(max_list),len(action)]) * (1/len(action)) # 方策[0.5,0.5]が1296個\n",
    "episode = np.zeros(step_num,np) # 方策によって決められた1エピソード分の行動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エージェントクラス(Agent Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,actions,policy,episode):\n",
    "        self.state = [] # 1エピソード分の状態配列，どこまで進んだか．\n",
    "        self.actions = actions # 各状態における行動(今回は1296個)\n",
    "        self.policy = policy # 方策[0.5,0.5]が全ての状態(今回は[1296,2])個\n",
    "        self.reward = [] # 1エピソード分の報酬\n",
    "        self.episode = episode # 方策によって決められた，各状態での行動(今回は100個)\n",
    "        \n",
    "    def set_state(self,state):\n",
    "        self.state = state # 現在の状態の更新\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def set_actions(self,actions):\n",
    "        self.actions = actions\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def set_policy(self,policy):\n",
    "        self.policy = policy \n",
    "    \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "    \n",
    "    def set_reward(self,reward):\n",
    "        self.reward = reward\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    def set_episode(self,episode):\n",
    "        self.episode = episode\n",
    "    \n",
    "    def get_episode(self):\n",
    "        return self.episode    \n",
    "    \n",
    "    def e_greedy(self,e=0.3): # 方策をすべてe-greedyする\n",
    "        policy = np.zeros(np.shape(self.get_policy()))\n",
    "        main_a = 1 - e + (e / len(self.get_policy()[0]))\n",
    "        other_a = e / len(self.get_policy()[0])\n",
    "        for i, action in enumerate(self.get_actions()):\n",
    "            policy[i] = [main_a,other_a] if action == 0 else [other_a,main_a]\n",
    "        self.set_policy(policy)\n",
    "        \n",
    "    def discretize(self,state, max_list, min_list, division):\n",
    "        disc_s = np.zeros(len(state),dtype=np.int32)\n",
    "        disc_s = [np.digitize(state[i], np.linspace(min_list[i], max_list[i], division-1)) for i in range(len(state))]\n",
    "        return disc_s\n",
    "        \n",
    "    def move(self,max_list,min_list,division,step_num): # moveで1エピソード分実行し，その時の状態と報酬を保存する\n",
    "        observation = env.reset() # cartpole環境の初期化\n",
    "        state_array = np.zeros([step_num+1,len(observation)])\n",
    "        state_array[0] = self.discretize(observation,max_list,min_list,division) # 現在の状態を離散化\n",
    "        reward_array = np.zeros(step_num+1)\n",
    "        episode_array = np.zeros(step_num+1,dtype=np.int32) # 1エピソードの行動を記憶する\n",
    "        for i in range(step_num):\n",
    "            # 現在の状態の位置\n",
    "            position = np.int(np.sum([state_array[i][j] * division ** j for j in range(len(state_array[i]))]))\n",
    "            # その時の方策から行動を決定\n",
    "            episode_array[i] = random.choices([0,1], k=1, weights=self.get_policy()[position])[0] \n",
    "            # 実行\n",
    "            observation, reward, done, info = env.step(episode_array[i])\n",
    "            # 現在の状態を離散化\n",
    "            state_array[i+1] = self.discretize(observation,max_list,min_list,division)\n",
    "            # 報酬の記憶\n",
    "            reward_array[i] = reward\n",
    "            if done == True:\n",
    "                break\n",
    "        self.set_state(state_array)\n",
    "        self.set_reward(reward_array)\n",
    "        self.set_episode(episode_array)\n",
    "        return i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境クラス(Environment Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> 環境クラスでは，エージェントクラスの呼び出し，エピソードの実行，結果の離散化，行動価値の算出，方策改善を行う．</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, actions, policy, episode, episode_num, step_num):\n",
    "        self.step_num = step_num\n",
    "        self.episode_num = episode_num\n",
    "        self.active_value = []\n",
    "        self.active_value_temp = []\n",
    "        self.active_value_count = []\n",
    "        \n",
    "        self.agent = Agent(actions,policy,episode)\n",
    "        \n",
    "    def set_active_value(self,active_value):\n",
    "        self.active_value = active_value\n",
    "        \n",
    "    def get_active_value(self):\n",
    "        return self.active_value\n",
    "        \n",
    "    def set_active_value_temp(self,active_value_temp):\n",
    "        self.active_value_temp = active_value_temp\n",
    "        \n",
    "    def get_active_value_temp(self):\n",
    "        return self.active_value_temp\n",
    "    \n",
    "    def set_active_value_count(self,active_value_count):\n",
    "        self.active_value_count = active_value_count\n",
    "        \n",
    "    def get_active_value_count(self):\n",
    "        return self.active_value_count\n",
    "    \n",
    "    def active_value_function(self,states,rewards,episode,div,count):\n",
    "        active_value = self.get_active_value() # 現在の行動価値の取得\n",
    "        active_value_temp = self.get_active_value_temp() # 平均を取るための一時的な箱\n",
    "        active_value_count = self.get_active_value_count() # 現在までのサンプリング数\n",
    "        temp_value = 0\n",
    "        true_state = states[:count]\n",
    "        for i,state in enumerate(reversed(true_state)): # 後ろから計算することで計算コストを抑える．式(2)を見て．\n",
    "            pos = np.int(np.sum([state[j] * div ** j for j in range(len(state))])) # ここで0~1295のどの状態かを特定する．    \n",
    "            # ----- ここから行動価値を算出する -----\n",
    "            temp = len(true_state)-(i+1) # 配列の後ろから処理を行う\n",
    "            temp_value = rewards[temp] + 0.9 * temp_value # 行動価値を代入\n",
    "            active_value_temp[pos,episode[temp],active_value_count[pos,episode[temp]]] = temp_value # 行動価値の可算            \n",
    "            active_value_count[pos,episode[temp]] += 1 # 行動価値の可算回数                \n",
    "            active_value[pos,episode[temp]] = np.sum(active_value_temp[pos,episode[temp]])/ \\\n",
    "                                              active_value_count[pos,episode[temp]]\n",
    "            # --------------------------------------\n",
    "            \n",
    "        self.set_active_value(active_value)\n",
    "        self.set_active_value_temp(active_value_temp)\n",
    "        self.set_active_value_count(active_value_count)\n",
    "        \n",
    "    def policy_improvement(self,loop):\n",
    "        active_values = self.get_active_value()\n",
    "        actions = self.agent.get_actions()\n",
    "        for i,active_value in enumerate(active_values):\n",
    "            actions[i] = 0 if active_value[0] > active_value[1] else 1        \n",
    "        self.agent.set_actions(actions)\n",
    "        self.agent.e_greedy(e = 0.3 if loop < 1500 else 0.1 )\n",
    "\n",
    "    def run(self, max_list, min_list, division):\n",
    "        count = np.zeros(self.episode_num, dtype=np.int32)\n",
    "        rewards_array = np.zeros(self.episode_num, dtype=np.int32)\n",
    "        \n",
    "        num_action = len(self.agent.get_policy()[0]) # 行動の数．2\n",
    "        self.set_active_value(np.zeros([division**len(max_list),num_action])) # [1296,2]\n",
    "        self.set_active_value_temp(np.zeros(\n",
    "            [division**len(max_list),num_action,self.episode_num*self.step_num])) # [1296,2,episode_num*step_num]\n",
    "        self.set_active_value_count(np.zeros([division**len(max_list),num_action],dtype=np.int32)) # [1296,2]\n",
    "        for i in range(self.episode_num):\n",
    "            count[i] = self.agent.move(max_list,min_list,division,self.step_num)\n",
    "            states = self.agent.get_state()\n",
    "            rewards = self.agent.get_reward()\n",
    "            self.active_value_function(states,rewards,self.agent.get_episode(),division,count[i])\n",
    "            self.policy_improvement(i)\n",
    "            rewards_array[i] = np.sum(rewards)\n",
    "        env.close()\n",
    "        return self.agent.get_policy(), rewards_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "経過時間：235.0470950603485s\n"
     ]
    }
   ],
   "source": [
    "save = os.getcwd() + \"/results\"\n",
    "\n",
    "env = gym.wrappers.Monitor(env, save, video_callable=(lambda ep: ep % 400 == 0), force = True)\n",
    "\n",
    "cartpole = Environment(actions, policy, episode, episode_num,step_num)\n",
    "\n",
    "t1 = time.time()\n",
    "optim_policy, optim_rewards = cartpole.run(max_list, min_list, division)\n",
    "t2 = time.time()\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}s\")\n",
    "\n",
    "with open(\"optim_policy.pickle\", 'wb') as rl:\n",
    "    pickle.dump(optim_policy , rl)\n",
    "    \n",
    "with open(\"optim_rewards.pickle\", 'wb') as rl:\n",
    "    pickle.dump(optim_rewards , rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"optim_policy.pickle\", 'rb') as rl:\n",
    "    optim_policy = pickle.load(rl)\n",
    "with open(\"optim_rewards.pickle\", 'rb') as rl:\n",
    "    optim_rewards = pickle.load(rl)\n",
    "\n",
    "def discretize(state, max_list, min_list, division):\n",
    "    disc_s = np.zeros(len(state),dtype=np.int32)\n",
    "    disc_s = [np.digitize(state[i], np.linspace(min_list[i], max_list[i], division-1)) for i in range(len(state))]\n",
    "    return disc_s\n",
    "    \n",
    "def show_cartpole(policy,max_list,min_list,division,step_num):\n",
    "    observation = env.reset() # cartpole環境の初期化\n",
    "    for i in range(step_num):\n",
    "        env.render()\n",
    "        # 現在の状態を離散化\n",
    "        observation = discretize(observation,max_list,min_list,division)\n",
    "        # 現在の状態の位置\n",
    "        position = np.int(np.sum([observation[j] * division ** j for j in range(len(observation))]))\n",
    "        # 実行\n",
    "        action = 0 if policy[position][0] > policy[position][1] else 1 \n",
    "        observation, reward, done, info = env.step(action)\n",
    "    observation = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期状態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "show_cartpole(policy,max_list,min_list,division,step_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7decebd780>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_cartpole(optim_policy,max_list,min_list,division,step_num)\n",
    "\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('return')\n",
    "plt.plot(optim_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> エピソードは多分，その都度状態とその状態時における方策によって決定されると思う．したがって，方策は[1296,2]のサイズが正しく，得られた状態はその場で離散化できるようにしなければいけないと考えられる．\n",
    "状態を得る→離散化→その時の方策→行動→終点まで繰り返す→行動価値→方策改善→...の流れだと思う．\n",
    "したがって，エピソードを予め作る必要はなく，その都度つくりその都度cartpoleの関数に行動として入力する．\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;\"> アルゴリズム的に正しく評価できているかわからない(自信がない)が，とりあえず棒を倒さないようにすることはできた．詳細は本を読んで確認しよう! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1, なんで方策$\\pi$の更新に状態価値関数じゃなくて行動価値関数を使うの? </span>  \n",
    "\n",
    "~~A1,正確には分からないが，今回のcartpoleの問題を考えると，重要なのは状態ではなくて行動ということがわかる．例えば，ポールの傾きが～，角速度が～，...の時の状態は〇〇ということよりも，土台を右にずらした時〇〇，左にずらした時〇〇ということが分かった方が制御がしやすいし考えやすくもある．また，何を更新すればよいかということも分かりやすい．状態だと，傾きを最適にしたいの?角速度を最適にしたいの?ってなる．今まではgrid worldを相手に考えていたので，行動よりも状態を重視しているところがあった．したがって，どのような問題かによって，状態価値関数と行動価値関数を使い分ける必要がある．~~  \n",
    "~~※ 遷移確率が使えないからという説明記事があったが，状態価値も行動価値も遷移確率を使わざる負えないので(式的に)，やはり問題によって使い分けるが適切だと考えている．モンテカルロ法は，遷移確率の代わりにサンプリングを用いている．多分...~~  \n",
    "\n",
    "A1, [最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c)にある通り，方策を改善するということは，状態価値がよりよくなるための<span style=\"color: red; \">行動$a$を選択する</span>ということ，つまり報酬が最大となるような行動$a$を選択すればよいということが分かる．そのためには，どの行動$a$を選択した場合に最も報酬が貰えるかを導出する必要がある．これは式で表すと，  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\pi^{*}(s,a) &=& \\mathrm{argmax}_{a} \\Sigma_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma \\mathrm{V^{\\pi}}(s')]， \\\\\n",
    "                 &=& \\mathrm{argmax}_{a} \\mathrm{Q^{\\pi}}(s,a)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    " \n",
    "となり，最も行動価値が大きくなる行動$a$を選択すればよいことがわかる．動的計画法(Dynamic Programming)では，このQ関数を計算により算出していたが，モンテカルロ法ではQ関数をサンプリングにより導出する．正しくは，$P^{a}_{ss'}$をサンプリングにより補う．Q関数の式はモンテカルロ法でも使うからね!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/policy_improvement.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [今さら聞けない強化学習(8): モンテカルロ法でOpenAI GymのCartpoleを学習](https://qiita.com/triwave33/items/1b9c87089b2fce0dd481)\n",
    "- [今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c)\n",
    "- [ε-greedy行動選択](https://www.tcom242242.net/entry/2017/01/15/163250/)\n",
    "- [強化学習について学んでみた。（その7）](https://yamaimo.hatenablog.jp/entry/2015/08/23/200000)\n",
    "- [Introducing CartPole-v1](https://subscription.packtpub.com/book/data/9781789345803/6/ch06lvl1sec47/introducing-cartpole-v1)\n",
    "- [OpenAI Gym](https://gym.openai.com/envs/#classic_control)\n",
    "- [第10回　CartPole課題をQ学習で制御する](https://book.mynavi.jp/manatee/detail/id=88997)\n",
    "- [【強化学習初心者向け】シンプルな実装例で学ぶSARSA法およびモンテカルロ法【CartPoleで棒立て：1ファイルで完結】](https://qiita.com/sugulu/items/7a14117bbd3d926eb1f2#%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92)\n",
    "- [強化学習について学んでみた。（その14）](https://yamaimo.hatenablog.jp/entry/2015/09/30/200000)\n",
    "- [【強化学習】まとめてみた　第五回（１）（Q学習だけじゃない！モンテカルロ法）【ブラックジャック攻略】](https://qiita.com/MENDY/items/4a896b9800775ad987a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
